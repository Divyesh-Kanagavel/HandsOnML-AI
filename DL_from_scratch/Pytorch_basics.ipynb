{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamental pytorch data type : the tensor \\\n",
    "it is basically an extension of the NumberWithGrad discussed but for a tensor [the torch equivalent of numpy array with Grad]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim import Optimizer\n",
    "\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.loss import _Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1.,2.,3.], requires_grad=True)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a * 4\n",
    "c = b + 3\n",
    "d = b * (a+5)\n",
    "e = c * 2\n",
    "e_sum = e.sum()\n",
    "e_sum.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.], requires_grad=True) tensor([ 4.,  8., 12.], grad_fn=<MulBackward0>) tensor([ 7., 11., 15.], grad_fn=<AddBackward0>) tensor([24., 56., 96.], grad_fn=<MulBackward0>) tensor([14., 22., 30.], grad_fn=<MulBackward0>) tensor(66., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(a,b,c,d,e,e_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8., 8., 8.])\n"
     ]
    }
   ],
   "source": [
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The building block of neural network : \n",
    "Layer, Model, Loss, Optimizer, Trainer can be written easily now using pytorch's nn module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_mode(model : nn.Module) -> None:\n",
    "    model.eval()\n",
    "\n",
    "class PytorchLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x : torch.Tensor, inference : bool = False) -> torch.Tensor :\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "# base pytorch model\n",
    "class PytorchModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x : torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(PytorchLayer):\n",
    "    def __init__(self, in_size : int, neurons : int, dropout : float = 1.,activation : nn.Module = None) :\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_size, neurons)\n",
    "        self.activation = activation\n",
    "        if dropout < 1.0:\n",
    "            self.dropout = nn.Dropout(1. - dropout)\n",
    "    \n",
    "    def forward(self, x : torch.Tensor, inference : bool = False) :\n",
    "        if inference :\n",
    "            self.apply(inference_mode)\n",
    "        x = self.linear(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        if hasattr(self, \"dropout\"):\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousePricesModel(PytorchModel):\n",
    "    def __init__(self,hidden_size : int = 8, hidden_dropout : float = 1.):\n",
    "        super().__init__()\n",
    "        self.dense1 = DenseLayer(8, hidden_size, hidden_dropout, nn.Sigmoid() )\n",
    "        self.dense2 = DenseLayer(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x : torch.Tensor):\n",
    "        assert(len(x.shape) == 2)\n",
    "        assert(x.shape[1] == 8)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "california_pytorch_model = HousePricesModel(hidden_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizers and losses are all coded up in pytorch and is easy to use\n",
    "optimizer = torch.optim.SGD(california_pytorch_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_loss = nn.MSELoss()\n",
    "softmax_crossentropy_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though softmax is not explicitly present in the crossEntropyLoss module, it performs softmax on the inputs and then calculates the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer class\n",
    "class PytorchTrainer(object):\n",
    "    def __init__(self, model : PytorchModel, optimizer : Optimizer, criterion : _Loss):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self._check_optim_net_aligned()\n",
    "    def _check_optim_net_aligned(self):\n",
    "        self.optimizer.param_groups[0][\"params\"] == list(self.model.parameters())\n",
    "\n",
    "    def _generate_batches(self, X: torch.Tensor, y : torch.Tensor, size : int = 32) -> Tuple[torch.Tensor]:\n",
    "        N = X.shape[0]\n",
    "        for i in range(0,N,size):\n",
    "            x_batch, y_batch = X[i:i+size], y[i:i+size]\n",
    "            yield x_batch, y_batch\n",
    "    \n",
    "    def fit(self, X_train:torch.Tensor, y_train:torch.Tensor, X_test:torch.Tensor, y_test:torch.Tensor,epochs : int =  100, eval_every : int = 10, batch_size : int = 32):\n",
    "        X_train, y_train = permute_data(X_train, y_train)\n",
    "        for e in range(epochs):\n",
    "            batch_generator = self._generate_batches(X_train, y_train, batch_size)\n",
    "            for i, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "                self.optimizer.zero_grad() # zero out the grads\n",
    "                y_pred = self.model(x_batch)\n",
    "                loss = self.criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            if e % eval_every == 0:\n",
    "                print(\"Epoch : {}, Loss : {}\".format(e,loss.item()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8) (20640,)\n"
     ]
    }
   ],
   "source": [
    "# train the model on california dataset\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "data = housing['data']\n",
    "target = housing['target']\n",
    "feature_names = housing['target_names']\n",
    "print(data.shape, target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "data = s.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target,test_size=0.3, random_state = 42)\n",
    "y_train, y_test = y_train.reshape(-1,1), y_test.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Loss : 0.4349268972873688\n",
      "Epoch : 1, Loss : 0.40333348512649536\n",
      "Epoch : 2, Loss : 0.3773728311061859\n",
      "Epoch : 3, Loss : 0.3564622104167938\n",
      "Epoch : 4, Loss : 0.33990156650543213\n",
      "Epoch : 5, Loss : 0.32696500420570374\n",
      "Epoch : 6, Loss : 0.3169691860675812\n",
      "Epoch : 7, Loss : 0.30931270122528076\n",
      "Epoch : 8, Loss : 0.30349117517471313\n",
      "Epoch : 9, Loss : 0.2990948557853699\n"
     ]
    }
   ],
   "source": [
    "trainer = PytorchTrainer(california_pytorch_model,optimizer, mean_squared_loss)\n",
    "trainer.fit(X_train,y_train,X_test,y_test,epochs=10,eval_every=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see a steady loss in loss which is a good sign. also, the model trained much faster compared to our numpy implementation maybe due to pytorch's optimization underneath [even on CPU cores]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tricks to Optimize Learning in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.optim.SGD(model.parameters(),lr=0.001,momentum=0.9) # for adding momentum to gradient update rule\n",
    "#nn.Dropout() for adding dropout with p percent of neurons dropped out\n",
    "# weigh initialization : already taken care of in pytorch Dense layer\n",
    "# from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional neural networks in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2DLayer(PytorchLayer):\n",
    "    def __init__(self, in_channels, out_channels, filter_size, activation, flatten=False, dropout=1.):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, filter_size, padding=filter_size//2)\n",
    "        self.activation = activation\n",
    "        self.flatten = flatten\n",
    "        if dropout < 1. :\n",
    "            self.dropout = nn.Dropout(1. - dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, X):\n",
    "\n",
    "        x = self.conv(X)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "\n",
    "        if self.flatten:\n",
    "            x = x.view(x.shape[0],-1)\n",
    "        if hasattr(self, \"dropout\"):\n",
    "            x = self.dropout(x)\n",
    "        return x\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST trainer class\n",
    "\n",
    "class MNISTConv(PytorchModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2DLayer(1,16,5,nn.Tanh(),dropout=0.8)\n",
    "        self.conv2 = Conv2DLayer(16,8,5,nn.Tanh(), flatten=True,dropout=0.8)\n",
    "        self.dense1 = DenseLayer(28*28*8, 32, activation=nn.Tanh(), dropout=0.8)\n",
    "        self.dense2 = DenseLayer(32,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dense1(x)\n",
    "        return self.dense2(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST dataset\n",
    "# the same MNIST dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape(-1,28*28)\n",
    "test_images = test_images.reshape(-1,28*28)\n",
    "\n",
    "X_train = (train_images - np.mean(train_images)) / np.std(train_images)\n",
    "X_test = (test_images - np.mean(test_images)) / np.std(test_images)\n",
    "y_train = np.zeros((train_labels.size, train_labels.max()+1))\n",
    "y_train[np.arange(train_labels.size), train_labels] = 1\n",
    "\n",
    "y_test = np.zeros((test_labels.size, train_labels.max()+1))\n",
    "y_test[np.arange(test_labels.size), test_labels] = 1\n",
    "\n",
    "X_train_conv = X_train.reshape(-1,1,28,28)\n",
    "X_test_conv = X_test.reshape(-1,1,28,28)\n",
    "\n",
    "X_train_conv = torch.from_numpy(X_train_conv.astype(np.float32))\n",
    "X_test_conv = torch.from_numpy(X_test_conv.astype(np.float32))\n",
    "\n",
    "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Loss : 0.09561838209629059\n",
      "Epoch : 1, Loss : 0.08975420892238617\n",
      "Epoch : 2, Loss : 0.061294347047805786\n",
      "Epoch : 3, Loss : 0.021063514053821564\n",
      "Epoch : 4, Loss : 0.01460924930870533\n"
     ]
    }
   ],
   "source": [
    "model = MNISTConv()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01,momentum=0.9)\n",
    "\n",
    "trainer = PytorchTrainer(model, optimizer, criterion)\n",
    "trainer.fit(X_train_conv, y_train, X_test_conv, y_test, epochs=5, eval_every=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model(X_test_conv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 10]), torch.Size([10000, 10]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test set is : 97.0 percent\n"
     ]
    }
   ],
   "source": [
    "acc = (torch.argmax(results,dim=1) == torch.argmax(y_test, dim=1)).sum() / 100\n",
    "print(\"accuracy on test set is : {} percent\".format(acc.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTMs in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(PytorchLayer):\n",
    "    def __init__(self,sequence_length, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.h_init = torch.zeros(1,self.hidden_size)\n",
    "        self.c_init = torch.zeros(1,self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.input_size, self.hidden_size, batch_first = True)\n",
    "        self.fc = DenseLayer(self.hidden_size,self.output_size)\n",
    "    \n",
    "    def _transform_hidden_batch(self, hidden, batch_size, before_layer = False):\n",
    "        if before_layer:\n",
    "            hidden.repeat(batch_size,1).view(batch_size, 1, self.hidden_size).permute(1,0,2)\n",
    "        else:\n",
    "            hidden.permute(1,0,2).mean(dim=0)\n",
    "    \n",
    "    \n",
    "    def forward(self,x):\n",
    "        batch_size = x.shape[0]\n",
    "        assert(x.shape[1] == self.sequence_length)\n",
    "        h_layer = self._transform_hidden_batch(self.h_init, batch_size, before_layer=True)\n",
    "        c_layer = self._transform_hidden_batch(self.c_init, batch_size, before_layer=True)\n",
    "\n",
    "        x = self.lstm(x, (h_layer, c_layer))\n",
    "        self.h_init = self._transform_hidden_batch(h_layer, batch_size, before_layer = False).detach()\n",
    "        self.c_init = self._transform_hidden_batch(c_layer, batch_size, before_layer = False).detach()\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way we can create a LSTM Layer which can be conveniently used in a LSTM model class and a LSTM trainer can be used to generate texts like how we did in the Recurrent_NN notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning in pytorch : Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in supervised learning, we have labels and the model is trained to first learn representations of patterns in data and then use these labels to steer the model to predict these labels correctly. In supervised learning, we do not have any labels, so then , how do we learn representations of patterns in data and then use these for any task like clustering, outlier detection etc. \\\n",
    "One way of doing it through autoencoders, where the labels is the training image itself and the model is required to reconstruct the image. the model has layers which increasingly encode the image into a lower dimensional representation and then a decoder reconstructs these latent representations into an approximation of the image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(PytorchModel):\n",
    "    def __init__(self, hidden_size : int = 28):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = Conv2DLayer(1, 14,5, activation=nn.Tanh() )\n",
    "        self.conv2 = Conv2DLayer(14, 7, 5,activation=nn.Tanh(),flatten=True )\n",
    "        self.dense1 = DenseLayer(28*28*7, hidden_size, activation=nn.Tanh())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        assert(len(x.shape)==4)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.dense1(x)\n",
    "        return x\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(PytorchModel):\n",
    "    def __init__(self, hidden_size = 28):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.dense1 = DenseLayer(hidden_size, 28*28*7,activation=nn.Tanh())\n",
    "        self.conv1 = Conv2DLayer(7, 14,5, activation=nn.Tanh())\n",
    "        self.conv2 = Conv2DLayer(14, 1, 5,activation=nn.Tanh())\n",
    "    \n",
    "    def forward(self,x):\n",
    "        assert(len(x.shape)==2)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = x.view(-1,7,28,28)\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are working with stride = 1, but when stride > 1, we will see a reduction is size with more and more convolution layers added.to retrieve back the image in input resolution, we do conv2d transpose which is supported in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(PytorchModel):\n",
    "    def __init__(self, hidden_size = 28):\n",
    "        super(AutoEncoder,self).__init__()\n",
    "        self.encoder = Encoder(hidden_size)\n",
    "        self.decoder = Decoder(hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        assert(len(x.shape) == 4)\n",
    "\n",
    "        encoding = self.encoder(x)\n",
    "\n",
    "        x = self.decoder(encoding)\n",
    "\n",
    "        return (x,encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer class\n",
    "class PytorchTrainer_updated(object):\n",
    "    def __init__(self, model : PytorchModel, optimizer : Optimizer, criterion : _Loss):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self._check_optim_net_aligned()\n",
    "    def _check_optim_net_aligned(self):\n",
    "        self.optimizer.param_groups[0][\"params\"] == list(self.model.parameters())\n",
    "\n",
    "    def _generate_batches(self, X: torch.Tensor, y : torch.Tensor, size : int = 32) -> Tuple[torch.Tensor]:\n",
    "        N = X.shape[0]\n",
    "        for i in range(0,N,size):\n",
    "            x_batch, y_batch = X[i:i+size], y[i:i+size]\n",
    "            yield x_batch, y_batch\n",
    "    \n",
    "    def fit(self, X_train:torch.Tensor, y_train:torch.Tensor, X_test:torch.Tensor, y_test:torch.Tensor,epochs : int =  100, eval_every : int = 10, batch_size : int = 32,final_lr_exp: int = None):\n",
    "        init_lr = self.optimizer.param_groups[0]['lr']\n",
    "        if final_lr_exp:\n",
    "            decay = (final_lr_exp / init_lr) ** (1.0 / (epochs + 1))\n",
    "            scheduler = lr_scheduler.ExponentialLR(self.optim, gamma=decay)\n",
    "        for e in range(epochs):\n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "            batch_generator = self._generate_batches(X_train, y_train, batch_size)\n",
    "            self.model.train() # trainin mode\n",
    "            for i, (x_batch, y_batch) in enumerate(batch_generator):\n",
    "                self.optimizer.zero_grad() # zero out the grads\n",
    "\n",
    "                y_pred = self.model(x_batch)[0]\n",
    "                loss = self.criterion(y_pred, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "            if e % eval_every == 0:\n",
    "                self.model.eval() # eval mode\n",
    "                output = self.model(X_test)[0]\n",
    "                loss = self.criterion(output, y_test)\n",
    "                print(\"Epoch : {}, Loss : {}\".format(e,loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in our model, we have a tanh activation after the nn.Linear() which squashes the output in the range of -1,1 but this is supposed to reconstruct our image which is in a different scale, so we convert our input image to this scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape(-1,28*28)\n",
    "test_images = test_images.reshape(-1,28*28)\n",
    "\n",
    "X_train = (train_images - np.mean(train_images)) / np.std(train_images)\n",
    "X_test = (test_images - np.mean(train_images)) / np.std(train_images)\n",
    "\n",
    "X_train_conv = X_train.reshape(-1,1,28,28)\n",
    "X_test_conv = X_test.reshape(-1,1,28,28)\n",
    "\n",
    "X_train_conv = torch.from_numpy(X_train_conv.astype(np.float32))\n",
    "X_test_conv = torch.from_numpy(X_test_conv.astype(np.float32))\n",
    "\n",
    "X_train_auto = (X_train_conv - X_train_conv.min()) / (X_train_conv.max() - X_train_conv.min()) * 2 - 1\n",
    "X_test_auto = (X_test_conv - X_train_conv.min()) / (X_train_conv.max() - X_train_conv.min()) * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28])\n",
      "torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(X_train_auto.shape)\n",
    "print(X_test_auto.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, Loss : 0.05582563579082489\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder(28)\n",
    "model.to(mps_device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
    "trainer = PytorchTrainer_updated(model, optimizer, criterion)\n",
    "\n",
    "trainer.fit(X_train_auto.to(mps_device), X_train_auto.to(mps_device), X_test_auto.to(mps_device), X_test_auto.to(mps_device), epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_images, image_representations = model(X_test_auto.to(mps_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1, 28, 28]), torch.Size([10000, 28]))"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_images.shape,image_representations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the image and the reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def display_image(ax,\n",
    "    t: Tensor):\n",
    "    n = t.detach().numpy()\n",
    "    ax.imshow(n.reshape(28, 28))\n",
    "\n",
    "np.random.seed(20190502)\n",
    "a = np.random.randint(0, 10000)\n",
    "\n",
    "f, axarr = plt.subplots(1,2)\n",
    "display_image(axarr[0], X_test_auto[a])\n",
    "display_image(axarr[1], reconstructed_images.cpu()[a])\n",
    "\n",
    "axarr[0].set_title(\"Original image\")\n",
    "axarr[1].set_title(\"Image reconstructed\\nfrom autoencoder\")\n",
    "\n",
    "axarr[0].axis('off')\n",
    "axarr[1].axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that the model is able to learn some latent representation from which it is able to get back the input image to a reasonable extent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
