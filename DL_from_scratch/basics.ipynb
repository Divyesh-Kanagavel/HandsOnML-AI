{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coding the fundamentals of DL - linear algebra, calculus from scratch. \\\n",
    "Consider input matrix X of shape [3,3] and weight matrix [3,2]. the resultant matrix N of shape [3,2] is passed through an activation function $\\sigma$ \\\n",
    "$S = \\sigma (N)$. the result is passed through a summation function $\\lambda$ which gives a scalar quantity L. \\\n",
    "the objective is to code backward pass computing gradient $\\frac{\\partial{L}}{\\partial{X}}$ and $\\frac{\\partial{L}}{\\partial{W}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deriv(func : Callable[[np.ndarray], np.ndarray],\n",
    "          input_ : np.ndarray,\n",
    "          delta : float = 1e-3) -> np.ndarray:\n",
    "    return ((func(input_ + delta) - func(input_ - delta)) / (2 * delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass\n",
    "def matrix_forward_sum(X : np.ndarray, W : np.ndarray, sigma ) -> np.ndarray:\n",
    "    assert X.shape[1] == W.shape[0] # matrix multiplicatin shape requirement\n",
    "    N = np.dot(X,W)\n",
    "    S = sigma(N)\n",
    "    L = np.sum(S)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some math, we see that :\n",
    " $\\frac{\\partial{L}}{\\partial{X}} = \\frac{\\partial{L}}{\\partial{\\sigma}} \\times \\frac{\\partial{L}}{\\partial{\\sigma}} \\times W^{T}$ \\\n",
    " this does not mean that $\\frac{\\partial{N}}{\\partial{X}} = W^{T}$, when you chain them together for this activation and matrix multiplication, you get $W^{T}$ in the expression simplifying the gradients. \\\n",
    " Similarly we note that :\n",
    " $\\frac{\\partial{L}}{\\partial{W}} = \\frac{\\partial{L}}{\\partial{\\sigma}} \\times X^{T} \\times \\frac{\\partial{\\sigma}}{\\partial{N}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_backward_sum(X : np.ndarray, W : np.ndarray, sigma) -> np.ndarray:\n",
    "    assert X.shape[1] == W.shape[0] # matrix multiplicatin shape requirement\n",
    "    N = np.dot(X,W)\n",
    "    S = sigma(N)\n",
    "    L = np.sum(S)\n",
    "\n",
    "    # backward pass\n",
    "    dL_ds = np.ones_like(S)\n",
    "    dS_dN = deriv(sigma, N)\n",
    "    dL_dN = dL_ds * dS_dN\n",
    "    dN_dX = np.transpose(W, (1,0))\n",
    "    dL_dX = np.dot(dL_dN, dN_dX)\n",
    "    return dL_dX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X : np.ndarray) -> np.ndarray:\n",
    "    return 1 / (1 + np.exp(-X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [[ 0.49671415 -0.1382643   0.64768854]\n",
      " [ 1.52302986 -0.23415337 -0.23413696]\n",
      " [ 1.57921282  0.76743473 -0.46947439]]\n",
      "W =  [[ 0.54256004 -0.46341769]\n",
      " [-0.46572975  0.24196227]\n",
      " [-1.91328024 -1.72491783]]\n",
      "L =  3.067006496510426\n",
      "dL_dX =  [[ 0.03685663 -0.05665209 -0.6691846 ]\n",
      " [-0.02524588 -0.01606117 -0.72432314]\n",
      " [-0.027645   -0.0145585  -0.72786339]]\n"
     ]
    }
   ],
   "source": [
    "# verification of code\n",
    "np.random.seed(42) # hitchhiker seed\n",
    "X = np.random.randn(3,3)\n",
    "W = np.random.randn(3,2)\n",
    "print(\"X = \", X)\n",
    "print(\"W = \", W)\n",
    "\n",
    "print(\"L = \", matrix_forward_sum(X,W,sigmoid))\n",
    "\n",
    "print(\"dL_dX = \", matrix_backward_sum(X,W,sigmoid))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dl_dx =  0.0368797434000534\n"
     ]
    }
   ],
   "source": [
    "#verficiation:\n",
    "# dL_dX's first component is 0.03685, which means if we increase X by 0.001, we should see \n",
    "# diff of 0.001 * 0.03685 in dL_dx\n",
    "X1 = X.copy()\n",
    "X1[0,0] += 0.001\n",
    "dL_dX_empirical = (matrix_forward_sum(X1,W, sigmoid) - matrix_forward_sum(X, W, sigmoid))/0.001\n",
    "print(\"dl_dx = \", dL_dX_empirical)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff =  -2.3113531009644195e-05\n"
     ]
    }
   ],
   "source": [
    "# diff between mathematical and empirical values\n",
    "print(\"diff = \", matrix_backward_sum(X,W,sigmoid)[0][0] - dL_dX_empirical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note :\n",
    "to compute, dL_dW , the expression is np.dot(dN_dW, dS_dN), where dN_dW is np.transpose(X, (1,0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
