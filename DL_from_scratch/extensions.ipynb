{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problems, we do not need to use MSE error like regression problems, there is a better loss function called the softmax cross entropy loss\n",
    "Softmax : is partial to the max of all values in a sequence. So, this leads to steeper gradients while training the model. also, it normalizes the sequence returning the probability values. \\\n",
    "Softmax is coupled with cross entropy loss which penalizes the model when we have lower probability values for target being 1 and higher probability values for target being 0. \\\n",
    "softmax(x) = $\\frac{e^{x_i}}{\\sum _i e^{x_i}}$ \\\n",
    "Cross entropy loss = $(y_i \\times \\log{(softmax(x))}) \\times ((1-y_i) \\times \\log{(1-softmax(x))})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    '''\n",
    "    Base class for an operation in a neural network\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_ : np.ndarray):\n",
    "        \n",
    "        '''\n",
    "        Stores input in the self.input attribute. \n",
    "        store output of forward computation is self.output attribute\n",
    "        '''\n",
    "\n",
    "        self.input_ = input_\n",
    "        self.output = self._output()\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Calls the self._input_grad() function\n",
    "        '''\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        assert(self.input_grad.shape == self.input_.shape)\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self) -> np.ndarray:\n",
    "        '''\n",
    "        the output method must be defined for each operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _input_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        the input_grad method must be defined for each operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    def __init__(self,param : np.ndarray) -> np.ndarray:\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "    \n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        assert(self.input_grad.shape == self.input_.shape)\n",
    "        assert(self.param_grad.shape == self.param.shape)\n",
    "\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _param_grad(self,output_grad : np.ndarray) -> np.ndarray :\n",
    "        raise NotImplementedError()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMultiply(ParamOperation):\n",
    "    def __init__(self, W : np.ndarray):\n",
    "        super().__init__(W)\n",
    "    \n",
    "    def _output(self) -> np.ndarray :\n",
    "        return np.dot(self.input_, self.param)\n",
    "        \n",
    "    def _input_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1,0)))\n",
    "    \n",
    "    def _param_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        return np.dot(np.transpose(self.input_,(1,0)), output_grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition of bias term\n",
    "class BiasAdd(ParamOperation):\n",
    "    def __init__(self,B:np.ndarray):\n",
    "        assert B.shape[0] == 1\n",
    "        super().__init__(B)\n",
    "    \n",
    "    def _output(self):\n",
    "        return self.input_ + self.param\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        return np.ones_like(self.input_) *  output_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, self.param.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation layer\n",
    "\n",
    "class Sigmoid(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self) -> np.ndarray:\n",
    "        return (1.0/(1.0+np.exp(-1.0 * self.input_)))\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        sigmoid_backward = self.output * (1-self.output) # derivative of sigmoid(x) = sigmoid(x) * (1-sigmoid(x))\n",
    "        return (sigmoid_backward * output_grad)\n",
    "\n",
    "class Linear(Operation):\n",
    "    def __init__(self) -> None :\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self) -> np.ndarray:\n",
    "        return self.input_\n",
    "    \n",
    "    def _input_grad(self, output_grad:np.ndarray) -> np.ndarray:\n",
    "        return output_grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract layer class\n",
    "class Layer(object):\n",
    "    def __init__(self, neurons : int) :\n",
    "        self.first = True # first layer or not\n",
    "        self.neurons = neurons\n",
    "        self.params : List[np.ndarray] = []\n",
    "        self.param_grads : List[np.ndarray] = []\n",
    "        self.operations : List[Operation] = []\n",
    "\n",
    "\n",
    "    \n",
    "    def _setup_layer(self, input_ : np.ndarray) -> None:\n",
    "        raise NotImplementedError() # to be filled in derived class\n",
    "    \n",
    "    def forward(self, input_ : np.ndarray) -> np.ndarray : \n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "            \n",
    "        self.input_ = input_\n",
    "        for operation in self.operations: \n",
    "            input_ = operation.forward(input_)\n",
    "        \n",
    "        self.output = input_\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "        \n",
    "        input_grad = output_grad\n",
    "        self._param_grads()\n",
    "        return input_grad\n",
    "    \n",
    "    def _param_grads(self):\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "            \n",
    "    def _params(self):\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, neurons : int, activation : Operation = Sigmoid()) -> None:\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "\n",
    "    \n",
    "    def _setup_layer(self, input_:np.ndarray) -> None:\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "        \n",
    "        self.params = []\n",
    "\n",
    "        self.params.append(np.random.randn(input_.shape[1],self.neurons))\n",
    "        self.params.append(np.random.randn(1,self.neurons))\n",
    "        self.operations = [weightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    '''\n",
    "    Loss function of the neural network\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, prediction : np.ndarray, target : np.ndarray) -> float :\n",
    "        assert(prediction.shape == target.shape)\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        loss_value = self._output()\n",
    "        return loss_value\n",
    "    \n",
    "    def backward(self) -> np.ndarray :\n",
    "        self.input_grad = self._input_grad()\n",
    "        assert(self.input_grad.shape == self.prediction.shape)\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _input_grad(self) -> np.ndarray:\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean squared loss : a subclass of Loss class\n",
    "\n",
    "class MeanSquaredLoss(Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self) -> float:\n",
    "        return np.sum(np.power((self.prediction-self.target),2)) / self.prediction.shape[0]\n",
    "    \n",
    "    def _input_grad(self) -> np.ndarray:\n",
    "        return 2.0 * (self.prediction-self.target) / self.prediction.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network class\n",
    "class NeuralNetwork(object):\n",
    "   def __init__(self,layers : List[Layer],\n",
    "                loss : Loss, seed : float = 1):\n",
    "      self.layers = layers\n",
    "      self.loss = loss\n",
    "      self.seed = seed\n",
    "      for layer in self.layers:\n",
    "         setattr(layer, \"seed\", self.seed)\n",
    "    \n",
    "   def forward(self, x_batch : np.ndarray) -> np.ndarray:\n",
    "      x_out = x_batch\n",
    "      for layer in self.layers:\n",
    "         x_out = layer.forward(x_out)\n",
    "      \n",
    "      return x_out\n",
    "\n",
    "   def backward(self, loss_grad : np.ndarray) -> None:\n",
    "      grad = loss_grad\n",
    "      for layer in reversed(self.layers):\n",
    "         grad = layer.backward(grad)\n",
    "   \n",
    "   def train_batch(self, X_batch : np.ndarray, y_batch : np.ndarray) -> float:\n",
    "      predictions = self.forward(X_batch)\n",
    "      loss = self.loss.forward(predictions, y_batch)\n",
    "      loss_grad = self.loss.backward()\n",
    "      self.backward(loss_grad)\n",
    "      return loss\n",
    "   \n",
    "   def params(self):\n",
    "      for layer in self.layers:\n",
    "         yield from layer.params # get the parameters from the generator instead of writing a loop and getting the params for the batch\n",
    "\n",
    "   def param_grads(self):\n",
    "      for layer in self.layers:\n",
    "         yield from layer.param_grads\n",
    "         \n",
    "\n",
    "\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next is the optimizer class\n",
    "# while training, we need to update the parameters based on gradients\n",
    "\n",
    "class Optimizer(object):\n",
    "    def __init__(self,lr = 0.001):\n",
    "        self.lr = lr\n",
    "        self.first = True\n",
    "    \n",
    "    def step(self) -> None :\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    def __init__(self, lr : float = 0.001):\n",
    "        super().__init__(lr)\n",
    "\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        for (param,param_grad) in zip(self.net.params(), self.net.param_grads()):\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "def mae(preds: np.ndarray, actuals: np.ndarray):\n",
    "    '''\n",
    "    Compute mean absolute error.\n",
    "    '''\n",
    "    return np.mean(np.abs(preds - actuals))\n",
    "\n",
    "def rmse(preds: np.ndarray, actuals: np.ndarray):\n",
    "    '''\n",
    "    Compute root mean squared error.\n",
    "    '''\n",
    "    return np.sqrt(np.mean(np.power(preds - actuals, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer class -> the class which implements the training loop\n",
    "from typing import Tuple\n",
    "from copy import deepcopy\n",
    "class Trainer(object):\n",
    "    def __init__(self, net : NeuralNetwork, optim : Optimizer) -> None:\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = np.inf # initial value for best loss\n",
    "        setattr(self.optim, \"net\",self.net)\n",
    "    \n",
    "    def generate_batches(self,X:np.ndarray, y:np.ndarray, batch_size : int = 32) -> Tuple[np.ndarray] :\n",
    "        assert X.shape[0] == y.shape[0] # shape check\n",
    "        N = X.shape[0]\n",
    "        for i in range(0,N,batch_size):\n",
    "            X_batch, y_batch = X[i:i+batch_size], y[i:i+batch_size]\n",
    "            yield X_batch, y_batch\n",
    "    \n",
    "    def fit(self, X_train : np.ndarray, y_train : np.ndarray, \n",
    "            X_test : np.ndarray, y_test : np.ndarray, batch_size : int = 32,\n",
    "            epochs : int = 100, eval_every : int = 100, seed : int = 42, restart : bool  = True) -> None:\n",
    "        np.random.seed(seed)\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "            \n",
    "            self.best_loss = np.inf\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            if (i+1)%eval_every == 0:\n",
    "                last_model = deepcopy(self.net)\n",
    "            \n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "            batch_generator = self.generate_batches(X_train, y_train)\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "                self.optim.step()\n",
    "            \n",
    "            if (i+1)%eval_every == 0:\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {i+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    self.net = last_model\n",
    "                    print(f\"\"\"Loss increased after epoch {i+1}, final loss was {self.best_loss:.3f}, using the model from epoch {i+1-eval_every}\"\"\")\n",
    "                    setattr(self.optim, \"net\", self.net)\n",
    "                    break\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all the building blocks from the previous notebook are copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(a : np.ndarray) -> np.ndarray :\n",
    "    other = 1-a\n",
    "    return np.concatenate([a,1-a], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(a : np.ndarray):\n",
    "    return a[np.newaxis,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "def softmax(x: np.ndarray, axis=None) -> np.ndarray:\n",
    "    return np.exp(x - special.logsumexp(x, axis=axis, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax cross entropy class\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    def __init__(self, eps: float = 1e-9) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.single_class = False\n",
    "\n",
    "    def _output(self) -> float:\n",
    "\n",
    "        # if the network is just outputting probabilities\n",
    "        # of just belonging to one class:\n",
    "        if self.target.shape[1] == 0:\n",
    "            self.single_class = True\n",
    "\n",
    "        # if \"single_class\", apply the \"normalize\" operation defined above:\n",
    "        if self.single_class:\n",
    "            self.prediction, self.target = normalize(self.prediction), normalize(\n",
    "                self.target\n",
    "            )\n",
    "\n",
    "        # applying the softmax function to each row (observation)\n",
    "        softmax_preds = softmax(self.prediction, axis=1)\n",
    "\n",
    "        # clipping the softmax output to prevent numeric instability\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "\n",
    "        # actual loss computation\n",
    "        softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - (\n",
    "            1.0 - self.target\n",
    "        ) * np.log(1 - self.softmax_preds)\n",
    "\n",
    "        return np.sum(softmax_cross_entropy_loss) / self.prediction.shape[0]\n",
    "\n",
    "    def _input_grad(self) -> np.ndarray:\n",
    "\n",
    "        # if \"single_class\", \"un-normalize\" probabilities before returning gradient:\n",
    "        if self.single_class:\n",
    "            return unnormalize(self.softmax_preds - self.target)\n",
    "        else:\n",
    "            return (self.softmax_preds - self.target) / self.prediction.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid activation function : introduces non-linearity to the model, but it has a downside, the gradients are flat when we reach extremes, near 0 or 1. So, the model does not have gradients to learn. as an alternative, another activation function called Rectified linear unit (ReLU) is used which has opposite strengths and weaknesses. the weakness is that it draws an arbitrary distinction between 0 and 1 and thus is not smooth , but it is compensated by other techniques. It produces larger gradients on average compared to sigmoid. 0.5 compared to 0.25 of sigmoid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is another activation function which is an intermediate between the two, which is the tanh activation function. it is used pretty often in deep learning models. the max gradient is 1 compared to sigmoid's 0.25 and also the function's differential is quite calculable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Activation functions have their own strenghs and drawbacks. To correct for Relu's drawbacks, there is the Leaky Relu which allows a little negative slope and thus allows flow of gradients backwards better. RELU6 caps the positive value at 6 introducing further non-linearity, but these are complex functions and for simple models, these are not required. for complex deeper models, these maybe employed after experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self) -> np.ndarray:\n",
    "        return np.tanh(self.input_)\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        tanh_backward = 1 - self.output * self.output # derivative of tanh(x) = (1-tanh(x)^2)\n",
    "        return (tanh_backward * output_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the benefit of using tanh activation along with Softmax cross entropy for classification problems, we use the famous MNIST digit classification dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,))"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing on images and labels : Feature scaling for images and one-hot encoding for labels\n",
    "train_images = train_images.reshape(-1,28*28)\n",
    "test_images = test_images.reshape(-1,28*28)\n",
    "\n",
    "# we do not scale the dataset to have mean =0 and unit variance using individual image means and variances , this would distort the image\n",
    "# instead we use the overall mean and variance\n",
    "# this way we have one mean and one variance for all pixels in image and they all get transformed by the same amout preserving the structural integrity\n",
    "# if we had used mean of pixel 1 across all images to normalize pixel1, it would alter pixel1 differently and pixel2 would be altered differently thereby destroying the intra-image structure\n",
    "\n",
    "X_train = (train_images - np.mean(train_images)) / np.std(train_images)\n",
    "X_test = (test_images - np.mean(test_images)) / np.std(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding labels\n",
    "\n",
    "y_train = np.zeros((train_labels.size, train_labels.max()+1))\n",
    "y_train[np.arange(train_labels.size), train_labels] = 1\n",
    "\n",
    "y_test = np.zeros((test_labels.size, train_labels.max()+1))\n",
    "y_test[np.arange(test_labels.size), test_labels] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 784), (10000, 10))"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape\n",
    "X_test.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us build a model with MSE loss function and sigmoid activation function. \\ \n",
    "the number of neurons chosen is our choice. but the thumb rule is to have n = $\\sqrt{n_{in} * n_{out}}$. So, h = sqrt(784*10) = 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = NeuralNetwork(layers = [Dense(neurons = 89, activation = Tanh()), Dense(10, activation=Sigmoid())],loss=MeanSquaredLoss(),seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGD(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.586\n",
      "Validation loss after 20 epochs is 0.440\n",
      "Validation loss after 30 epochs is 0.362\n",
      "Validation loss after 40 epochs is 0.347\n",
      "Validation loss after 50 epochs is 0.342\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model1, optimizer)\n",
    "trainer.fit(X_train, y_train, X_test,y_test,batch_size=64,epochs=50, eval_every=10,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy_model(model, test_set):\n",
    "    return print(\n",
    "        '''The model validation accuracy is: {0:.2f}%'''.format(\n",
    "            np.equal(np.argmax(model.forward(test_set), axis=1), test_labels).sum()\n",
    "            * 100.0\n",
    "            / test_set.shape[0]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model validation accuracy is: 73.79%\n"
     ]
    }
   ],
   "source": [
    "calc_accuracy_model(model1, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = NeuralNetwork(\n",
    "    layers = [Dense(neurons=89, activation=Tanh()), Dense(neurons=10,activation=Linear())],\n",
    "    loss = SoftmaxCrossEntropy(), seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.548\n",
      "Validation loss after 20 epochs is 0.514\n",
      "Validation loss after 30 epochs is 0.508\n",
      "Validation loss after 40 epochs is 0.507\n",
      "Loss increased after epoch 50, final loss was 0.507, using the model from epoch 40\n"
     ]
    }
   ],
   "source": [
    "optimizer = SGD(0.1)\n",
    "trainer = Trainer(model2, optimizer)\n",
    "trainer.fit(X_train, y_train, X_test,y_test,batch_size=64,epochs=50, eval_every=10,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model validation accuracy is: 91.87%\n"
     ]
    }
   ],
   "source": [
    "calc_accuracy_model(model2, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with sigmoid activation function + MSE , we got an accuracy of 73.9 percent after 50 epochs. but for the softmax cross entropy loss, we see an accuracy of 91.87 percent which is way higher than the one with MSE. So, take-away lesson is that a careful selection of loss function can help train the model more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum : So, we have been using the gradients to update the weights. But, we can incorporate momentum into the weight update. it is based on the concept in physics that velocity does not depend on the applied forces, but also the past velocities. Hence we keep track of the change in gradients in the past and use that to update the gradients. \\\n",
    "Mathematical equation : \n",
    "update = $del _t + \\mu \\times del _{t-1} + \\mu^2 \\times del_{t-2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(Optimizer):\n",
    "    def __init__(\n",
    "        self, lr: float = 0.01, momentum: float = 0.9\n",
    "    ) -> None:\n",
    "        super().__init__(lr)\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def step(self) -> None:\n",
    "        if self.first:\n",
    "            self.velocities = [np.zeros_like(param) for param in self.net.params()]\n",
    "            self.first = False\n",
    "\n",
    "        for param, param_grad, velocity in zip(\n",
    "            self.net.params(), self.net.param_grads(), self.velocities\n",
    "        ):\n",
    "            self._update_rule(param=param, grad=param_grad, velocity=velocity)\n",
    "\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "\n",
    "        # Update velocity\n",
    "        kwargs[\"velocity\"] *= self.momentum\n",
    "        kwargs[\"velocity\"] += self.lr * kwargs[\"grad\"]\n",
    "\n",
    "        # Use this to update parameters\n",
    "        kwargs[\"param\"] -= kwargs[\"velocity\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model with same specs as before but with SGDMomentum optimzier instead of SGD\n",
    "model3 = NeuralNetwork(\n",
    "    layers=[\n",
    "        Dense(neurons=89, activation=Tanh()),\n",
    "        Dense(neurons=10, activation=Linear()),\n",
    "    ],\n",
    "    loss=SoftmaxCrossEntropy(),\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = SGDMomentum(lr = 0.1, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.537\n",
      "Validation loss after 20 epochs is 0.358\n",
      "Validation loss after 30 epochs is 0.304\n",
      "Loss increased after epoch 40, final loss was 0.304, using the model from epoch 30\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model3, optimizer2)\n",
    "trainer.fit(X_train, y_train, X_test,y_test,batch_size=64,epochs=50, eval_every=10,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model validation accuracy is: 94.91%\n"
     ]
    }
   ],
   "source": [
    "calc_accuracy_model(model3, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the addition of momentum in our gradient update rule, we see an improvement in accuracy from 91.87% to 94.91 % which is quite a jump. \\\n",
    "We will now try to set an adaptible learning rate instead of a fixed learning rate and see if there is an improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next step is to implement learning rate decay\n",
    "# this is incorporated inside the optimizer\n",
    "class Optimizer2(object):\n",
    "    def __init__(self, initial_lr : float = 0.01, final_lr : float = 0., decay_type : str = \"exponential\"):\n",
    "        self.lr = initial_lr\n",
    "        self.final_lr = final_lr\n",
    "        self.decay_type = decay_type\n",
    "        self.first = True\n",
    "    def step(self):\n",
    "        pass\n",
    "\n",
    "    def _setup_decay(self) -> None:\n",
    "        if not self.decay_type:\n",
    "            return\n",
    "        elif self.decay_type == \"exponential\":\n",
    "            self.decay_per_epoch = np.power(self.final_lr/self.lr, 1.0/(self.max_epochs-1))\n",
    "        \n",
    "        elif self.decay_type == \"linear\":\n",
    "            self.decay_per_epoch = (self.lr - self.final_lr) / (self.max_epochs - 1)\n",
    "    \n",
    "    def _decay_lr(self) -> None : \n",
    "        if not self.decay_type:\n",
    "            return\n",
    "        elif self.decay_type == \"exponential\":\n",
    "            self.lr *= self.decay_per_epoch\n",
    "        elif self.decay_type == \"linear\":\n",
    "            self.lr -= self.decay_per_epoch\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the new optimizer class, we will now have to modify other classes a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD2(Optimizer2):\n",
    "    def __init__(self, lr : float = 0.001, final_lr : float = 0., decay_type : str = \"exponential\"):\n",
    "        super().__init__(lr, final_lr, decay_type)\n",
    "\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        for (param,param_grad) in zip(self.net.params(), self.net.param_grads()):\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum2(Optimizer2):\n",
    "    def __init__(\n",
    "        self, lr: float = 0.01, final_lr : float = 0. , decay_type : str = \"exponential\", momentum: float = 0.9\n",
    "    ) -> None:\n",
    "        super().__init__(lr, final_lr,decay_type)\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def step(self) -> None:\n",
    "        if self.first:\n",
    "            self.velocities = [np.zeros_like(param) for param in self.net.params()]\n",
    "            self.first = False\n",
    "\n",
    "        for param, param_grad, velocity in zip(\n",
    "            self.net.params(), self.net.param_grads(), self.velocities\n",
    "        ):\n",
    "            self._update_rule(param=param, grad=param_grad, velocity=velocity)\n",
    "\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "\n",
    "        # Update velocity\n",
    "        kwargs[\"velocity\"] *= self.momentum\n",
    "        kwargs[\"velocity\"] += self.lr * kwargs[\"grad\"]\n",
    "\n",
    "        # Use this to update parameters\n",
    "        kwargs[\"param\"] -= kwargs[\"velocity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainer class -> the class which implements the training loop\n",
    "from typing import Tuple\n",
    "from copy import deepcopy\n",
    "class Trainer2(object):\n",
    "    def __init__(self, net : NeuralNetwork, optim : Optimizer2) -> None:\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = np.inf # initial value for best loss\n",
    "        setattr(self.optim, \"net\",self.net)\n",
    "\n",
    "    \n",
    "    def generate_batches(self,X:np.ndarray, y:np.ndarray, batch_size : int = 32) -> Tuple[np.ndarray] :\n",
    "        assert X.shape[0] == y.shape[0] # shape check\n",
    "        N = X.shape[0]\n",
    "        for i in range(0,N,batch_size):\n",
    "            X_batch, y_batch = X[i:i+batch_size], y[i:i+batch_size]\n",
    "            yield X_batch, y_batch\n",
    "    \n",
    "    def fit(self, X_train : np.ndarray, y_train : np.ndarray, \n",
    "            X_test : np.ndarray, y_test : np.ndarray, batch_size : int = 32,\n",
    "            epochs : int = 100, eval_every : int = 100, seed : int = 42, restart : bool  = True) -> None:\n",
    "        np.random.seed(seed)\n",
    "        setattr(self.optim, \"max_epochs\", epochs)\n",
    "        self.optim._setup_decay()\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "            \n",
    "            self.best_loss = np.inf\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            if (i+1)%eval_every == 0:\n",
    "                last_model = deepcopy(self.net)\n",
    "            \n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "            batch_generator = self.generate_batches(X_train, y_train)\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "                self.optim.step()\n",
    "            if self.optim.final_lr:\n",
    "                self.optim._decay_lr()\n",
    "            \n",
    "            if (i+1)%eval_every == 0:\n",
    "                test_preds = self.net.forward(X_test)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {i+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    self.net = last_model\n",
    "                    print(f\"\"\"Loss increased after epoch {i+1}, final loss was {self.best_loss:.3f}, using the model from epoch {i+1-eval_every}\"\"\")\n",
    "                    setattr(self.optim, \"net\", self.net)\n",
    "                    break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGDMomentum2(0.15,0.05,\"linear\", 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[309], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer2(model3, optimizer)\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, X_test,y_test,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, eval_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "Cell \u001b[0;32mIn[307], line 38\u001b[0m, in \u001b[0;36mTrainer2.fit\u001b[0;34m(self, X_train, y_train, X_test, y_test, batch_size, epochs, eval_every, seed, restart)\u001b[0m\n\u001b[1;32m     36\u001b[0m batch_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_batches(X_train, y_train)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii, (X_batch, y_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_generator):\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet\u001b[38;5;241m.\u001b[39mtrain_batch(X_batch, y_batch)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mfinal_lr:\n",
      "Cell \u001b[0;32mIn[287], line 24\u001b[0m, in \u001b[0;36mNeuralNetwork.train_batch\u001b[0;34m(self, X_batch, y_batch)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_batch\u001b[39m(\u001b[38;5;28mself\u001b[39m, X_batch : np\u001b[38;5;241m.\u001b[39mndarray, y_batch : np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m---> 24\u001b[0m    predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(X_batch)\n\u001b[1;32m     25\u001b[0m    loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mforward(predictions, y_batch)\n\u001b[1;32m     26\u001b[0m    loss_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[287], line 14\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x_batch)\u001b[0m\n\u001b[1;32m     12\u001b[0m x_out \u001b[38;5;241m=\u001b[39m x_batch\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 14\u001b[0m    x_out \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mforward(x_out)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x_out\n",
      "Cell \u001b[0;32mIn[283], line 22\u001b[0m, in \u001b[0;36mLayer.forward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ \u001b[38;5;241m=\u001b[39m input_\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m operation \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperations: \n\u001b[0;32m---> 22\u001b[0m     input_ \u001b[38;5;241m=\u001b[39m operation\u001b[38;5;241m.\u001b[39mforward(input_)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m input_\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
      "Cell \u001b[0;32mIn[278], line 16\u001b[0m, in \u001b[0;36mOperation.forward\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mStores input in the self.input attribute. \u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03mstore output of forward computation is self.output attribute\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_ \u001b[38;5;241m=\u001b[39m input_\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput\n",
      "Cell \u001b[0;32mIn[280], line 6\u001b[0m, in \u001b[0;36mweightMultiply._output\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_output\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray :\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mdot(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer2(model3, optimizer)\n",
    "trainer.fit(X_train, y_train, X_test,y_test,batch_size=64,epochs=50, eval_every=10,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model validation accuracy is: 96.02%\n"
     ]
    }
   ],
   "source": [
    "calc_accuracy_model(model3, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer2 = SGDMomentum2(0.2,0.05,\"exponential\", 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.783\n",
      "Validation loss after 20 epochs is 0.504\n",
      "Validation loss after 30 epochs is 0.385\n",
      "Validation loss after 40 epochs is 0.345\n",
      "Validation loss after 50 epochs is 0.330\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer2(model3, optimizer2)\n",
    "trainer.fit(X_train, y_train, X_test,y_test,batch_size=64,epochs=50, eval_every=10,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model validation accuracy is: 95.24%\n"
     ]
    }
   ],
   "source": [
    "calc_accuracy_model(model3, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with an adaptible learning rate, we see that there is a reduction in validation loss with both linear and exponential decay, Also the accuracy has increased to 96.02 percent which is cool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have added two optimizations to the model : momentum in gradient update, learning rate decay. the next optimization which helps deep learning model perform better is better weight initialization. currently we are initializing the weight of to have unit variance. in mnist dataset, we are also normalizing the inputs. but, after the first matrix multiplication, assuming our input features are independent, we see that the var_out = var_in1 + var_in2 + ... var_nin. \\\n",
    "So, if we have 785 input values, after the first layer matrix multiply and bias add, we get standard deviation to be about 28. So, the values after matrix multiplication operation are spread out and these are then passed to activation functions which squash these values within a range , say -1 to 1. So, most of the values are around 1 or -1 and this is undesirable [especially for larger deep learning models.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are quite a few methods to solve this problem. one of the prominent ones is clever initialization of weights. this way, we see that the spread of values problem can be mitigated. We also need to worry about the variance of gradients flowing back from the end. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the input neurons is $n_{in}$ and the number of output neurons is $n_{out}$, to scale the variance during forward pass, we need to multiply the scale factor $\\frac{1}{n_{in}}$ and during the backward pass, the gradients need to be scaled by $\\frac{1}{n_{out}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense2(Layer):\n",
    "\n",
    "    def __init__(self, neurons : int, activation : Operation = Sigmoid(), weight_init = \"glorot\") -> None:\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "        self.weight_init = weight_init\n",
    "\n",
    "\n",
    "    \n",
    "    def _setup_layer(self, input_:np.ndarray) -> None:\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        if self.weight_init == \"glorot\":\n",
    "            scale = 2 / (input_.shape[1] + self.neurons)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        \n",
    "        self.params = []\n",
    "\n",
    "        self.params.append(scale * np.random.randn(input_.shape[1],self.neurons))\n",
    "        self.params.append(np.random.randn(1,self.neurons))\n",
    "        self.operations = [weightMultiply(self.params[0]),\n",
    "                           BiasAdd(self.params[1]),\n",
    "                           self.activation]\n",
    "\n",
    "        \n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model with glorot weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = NeuralNetwork(\n",
    "    layers=[\n",
    "        Dense2(neurons=89, activation=Tanh(), weight_init = \"glorot\" ),\n",
    "        Dense2(neurons=10, activation=Linear(), weight_init = \"glorot\"),\n",
    "    ],\n",
    "    loss=SoftmaxCrossEntropy(),\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.246\n",
      "Validation loss after 20 epochs is 0.217\n",
      "Loss increased after epoch 30, final loss was 0.217, using the model from epoch 20\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer2(model4, optimizer2)\n",
    "trainer.fit(X_train, y_train, X_test,y_test,batch_size=64,epochs=50, eval_every=10,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model validation accuracy is: 96.90%\n"
     ]
    }
   ],
   "source": [
    "calc_accuracy_model(model4, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that the loss has reduced to 0.217 and the accuracy has increased to 96.55 percent with the improvements in weight initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one final optimization : Dropout \\\n",
    "When we try to build a deeper model with more number of layers, neurons per layer etc, the performance worsens. this is because the model overfits on the training data and thus does not generalize well to unseen data. To prevent this, we do something called dropout. i.e we dropout a proportion of neurons from learning during training. this way, there is less probability of the model overfitting on the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During inference, we do not want a proportion of neurons to miss out from firing, so we use all the neurons. But since we disabled p percent neurons from participating during training, the magnitude of values being passed forward was M * (1-p) instead of usual M. To simulate this drop in magnitude, we multiple by (1-p) with all neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us now code all the classes with support of SGD Momentum, Weight initialization, learning rate decay and Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation_final(object):\n",
    "    '''\n",
    "    Base class for an operation in a neural network\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_ : np.ndarray, inference : bool = False):\n",
    "        \n",
    "        '''\n",
    "        Stores input in the self.input attribute. \n",
    "        store output of forward computation is self.output attribute\n",
    "        '''\n",
    "        self.input_ = input_\n",
    "\n",
    "        self.output = self._output(inference)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Calls the self._input_grad() function\n",
    "        '''\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        assert(self.input_grad.shape == self.input_.shape)\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray:\n",
    "        '''\n",
    "        the output method must be defined for each operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _input_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        the input_grad method must be defined for each operation\n",
    "        '''\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation_final(Operation_final):\n",
    "    def __init__(self,param : np.ndarray) -> np.ndarray:\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "    \n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        assert(self.input_grad.shape == self.input_.shape)\n",
    "        assert(self.param_grad.shape == self.param.shape)\n",
    "\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _param_grad(self,output_grad : np.ndarray) -> np.ndarray :\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMultiply_final(ParamOperation_final):\n",
    "    def __init__(self, W : np.ndarray):\n",
    "        super().__init__(W)\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray :\n",
    "        return np.dot(self.input_, self.param)\n",
    "        \n",
    "    def _input_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1,0)))\n",
    "    \n",
    "    def _param_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        return np.dot(np.transpose(self.input_,(1,0)), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition of bias term\n",
    "class BiasAdd_final(ParamOperation_final):\n",
    "    def __init__(self,B:np.ndarray):\n",
    "        assert B.shape[0] == 1\n",
    "        super().__init__(B)\n",
    "    \n",
    "    def _output(self, inference : bool = False):\n",
    "        return self.input_ + self.param\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        return np.ones_like(self.input_) *  output_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, self.param.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation layer\n",
    "\n",
    "class Sigmoid_final(Operation_final):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray:\n",
    "        return (1.0/(1.0+np.exp(-1.0 * self.input_)))\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        sigmoid_backward = self.output * (1-self.output) # derivative of sigmoid(x) = sigmoid(x) * (1-sigmoid(x))\n",
    "        return (sigmoid_backward * output_grad)\n",
    "\n",
    "class Linear_final(Operation_final):\n",
    "    def __init__(self) -> None :\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray:\n",
    "        return self.input_\n",
    "    \n",
    "    def _input_grad(self, output_grad:np.ndarray) -> np.ndarray:\n",
    "        return output_grad\n",
    "    \n",
    "class Tanh_final(Operation_final):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray:\n",
    "        return np.tanh(self.input_)\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        tanh_backward = 1 - self.output * self.output # derivative of tanh(x) = (1-tanh(x)^2)\n",
    "        return (tanh_backward * output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dropout(Operation_final):\n",
    "    def __init__(self, keep_prob : float = 0.8):\n",
    "        super().__init__()\n",
    "        self.keep_prob = keep_prob\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray:\n",
    "        if inference : \n",
    "            return self.input_ * self.keep_prob\n",
    "        else:\n",
    "            self.mask = np.random.binomial(1, self.keep_prob, size=self.input_.shape)\n",
    "            return self.input_ * self.mask \n",
    "    \n",
    "    def _input_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        return output_grad * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract layer class\n",
    "class Layer_final(object):\n",
    "    def __init__(self, neurons : int, dropout : float = 1.) :\n",
    "        self.first = True # first layer or not\n",
    "        self.neurons = neurons\n",
    "        self.params : List[np.ndarray] = []\n",
    "        self.param_grads : List[np.ndarray] = []\n",
    "        self.operations : List[Operation_final] = []\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "    \n",
    "    def _setup_layer(self, input_ : np.ndarray) -> None:\n",
    "        raise NotImplementedError() # to be filled in derived class\n",
    "    \n",
    "    def forward(self, input_ : np.ndarray, inference : bool = False) -> np.ndarray : \n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "            \n",
    "        self.input_ = input_\n",
    "        \n",
    "        for operation in self.operations: \n",
    "            \n",
    "            input_ = operation.forward(input_, inference)\n",
    "        \n",
    "        self.output = input_\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "        \n",
    "        input_grad = output_grad\n",
    "        self._param_grads()\n",
    "        return input_grad\n",
    "    \n",
    "    def _param_grads(self):\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation_final):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "            \n",
    "    def _params(self):\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation_final):\n",
    "                self.params.append(operation.param)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_final(Layer_final):\n",
    "    def __init__(self, neurons : int, activation : Operation_final = Sigmoid_final(), weight_init = \"glorot\", dropout : float = 1.) -> None:\n",
    "        super().__init__(neurons, dropout)\n",
    "        self.activation = activation\n",
    "        self.weight_init = weight_init\n",
    "\n",
    "\n",
    "    \n",
    "    def _setup_layer(self, input_:np.ndarray) -> None:\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        if self.weight_init == \"glorot\":\n",
    "            scale = 2 / (input_.shape[1] + self.neurons)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        \n",
    "        self.params = []\n",
    "\n",
    "        self.params.append(scale * np.random.randn(input_.shape[1],self.neurons))\n",
    "        self.params.append(np.random.randn(1,self.neurons))\n",
    "        self.operations = [weightMultiply_final(self.params[0]),\n",
    "                           BiasAdd_final(self.params[1]),\n",
    "                           self.activation]\n",
    "        if self.dropout < 1.:\n",
    "            self.operations.append(Dropout(self.dropout))\n",
    "\n",
    "        \n",
    "        return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network class\n",
    "class NeuralNetwork_final(object):\n",
    "   def __init__(self,layers : List[Layer_final],\n",
    "                loss : Loss, seed : float = 1):\n",
    "      self.layers = layers\n",
    "      self.loss = loss\n",
    "      self.seed = seed\n",
    "      for layer in self.layers:\n",
    "         setattr(layer, \"seed\", self.seed)\n",
    "    \n",
    "   def forward(self, x_batch : np.ndarray, inference : bool = False) -> np.ndarray:\n",
    "      x_out = x_batch\n",
    "\n",
    "      for layer in self.layers:\n",
    "         x_out = layer.forward(x_out,inference)\n",
    "      \n",
    "      return x_out\n",
    "\n",
    "   def backward(self, loss_grad : np.ndarray) -> None:\n",
    "      grad = loss_grad\n",
    "      for layer in reversed(self.layers):\n",
    "         grad = layer.backward(grad)\n",
    "   \n",
    "   def train_batch(self, X_batch : np.ndarray, y_batch : np.ndarray) -> float:\n",
    "      predictions = self.forward(X_batch)\n",
    "      loss = self.loss.forward(predictions, y_batch)\n",
    "      loss_grad = self.loss.backward()\n",
    "      self.backward(loss_grad)\n",
    "      return loss\n",
    "   \n",
    "   def params(self):\n",
    "      for layer in self.layers:\n",
    "         yield from layer.params # get the parameters from the generator instead of writing a loop and getting the params for the batch\n",
    "\n",
    "   def param_grads(self):\n",
    "      for layer in self.layers:\n",
    "         yield from layer.param_grads\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer_final(object):\n",
    "    def __init__(self, net : NeuralNetwork_final, optim : Optimizer2) -> None:\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = np.inf # initial value for best loss\n",
    "        setattr(self.optim, \"net\",self.net)\n",
    "\n",
    "    \n",
    "    def generate_batches(self,X:np.ndarray, y:np.ndarray, batch_size : int = 32) -> Tuple[np.ndarray] :\n",
    "        assert X.shape[0] == y.shape[0] # shape check\n",
    "        N = X.shape[0]\n",
    "        for i in range(0,N,batch_size):\n",
    "            X_batch, y_batch = X[i:i+batch_size], y[i:i+batch_size]\n",
    "            yield X_batch, y_batch\n",
    "    \n",
    "    def fit(self, X_train : np.ndarray, y_train : np.ndarray, \n",
    "            X_test : np.ndarray, y_test : np.ndarray, batch_size : int = 32,\n",
    "            epochs : int = 100, eval_every : int = 100, seed : int = 42, restart : bool  = True) -> None:\n",
    "        np.random.seed(seed)\n",
    "        setattr(self.optim, \"max_epochs\", epochs)\n",
    "        self.optim._setup_decay()\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "            \n",
    "            self.best_loss = np.inf\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            if (i+1)%eval_every == 0:\n",
    "                last_model = deepcopy(self.net)\n",
    "            \n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "            batch_generator = self.generate_batches(X_train, y_train)\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "                self.optim.step()\n",
    "            if self.optim.final_lr:\n",
    "                self.optim._decay_lr()\n",
    "            \n",
    "            if (i+1)%eval_every == 0:\n",
    "                test_preds = self.net.forward(X_test, inference = True)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {i+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    self.net = last_model\n",
    "                    print(f\"\"\"Loss increased after epoch {i+1}, final loss was {self.best_loss:.3f}, using the model from epoch {i+1-eval_every}\"\"\")\n",
    "                    setattr(self.optim, \"net\", self.net)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = NeuralNetwork_final(\n",
    "    layers=[\n",
    "        Dense_final(neurons=89, activation=Tanh_final(), weight_init = \"glorot\",dropout = 0.8 ),\n",
    "        Dense_final(neurons=10, activation=Linear_final(), weight_init = \"glorot\"),\n",
    "    ],\n",
    "    loss=SoftmaxCrossEntropy(),\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.198\n",
      "Validation loss after 20 epochs is 0.179\n",
      "Validation loss after 30 epochs is 0.175\n",
      "Validation loss after 40 epochs is 0.172\n",
      "Loss increased after epoch 50, final loss was 0.172, using the model from epoch 40\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer_final(model_dropout, optimizer2)\n",
    "trainer.fit(X_train, y_train, X_test,y_test,batch_size=64,epochs=50, eval_every=10,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model validation accuracy is: 96.02%\n"
     ]
    }
   ],
   "source": [
    "calc_accuracy_model(model_dropout, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let us try a bigger model -> dropout helps train bigger models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout_big = NeuralNetwork_final(\n",
    "    layers=[\n",
    "        Dense_final(neurons=178, activation=Tanh_final(), weight_init = \"glorot\",dropout = 0.8 ),\n",
    "        Dense_final(neurons=46, activation=Tanh_final(), weight_init = \"glorot\",dropout = 0.8 ),\n",
    "        Dense_final(neurons=10, activation=Linear_final(), weight_init = \"glorot\"),\n",
    "    ],\n",
    "    loss=SoftmaxCrossEntropy(),\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss after 10 epochs is 0.204\n",
      "Validation loss after 20 epochs is 0.198\n",
      "Validation loss after 30 epochs is 0.171\n",
      "Validation loss after 40 epochs is 0.168\n",
      "Validation loss after 50 epochs is 0.165\n",
      "Validation loss after 60 epochs is 0.162\n",
      "Validation loss after 70 epochs is 0.155\n",
      "Validation loss after 80 epochs is 0.153\n",
      "Validation loss after 90 epochs is 0.149\n",
      "Loss increased after epoch 100, final loss was 0.149, using the model from epoch 90\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer_final(model_dropout_big, optimizer2)\n",
    "trainer.fit(X_train, y_train, X_test,y_test,batch_size=64,epochs=100, eval_every=10,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model validation accuracy is: 96.49%\n"
     ]
    }
   ],
   "source": [
    "calc_accuracy_model(model_dropout_big, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we seen an improvement over the smaller models with dropout.  we could try with different dropout rates!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
