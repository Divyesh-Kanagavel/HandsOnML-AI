{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building blocks of computations which can be reused to construct deeper complex DL \n",
    "models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Callable, List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    '''\n",
    "    Base class for an operation in a neural network\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_ : np.ndarray):\n",
    "        \n",
    "        '''\n",
    "        Stores input in the self.input attribute. \n",
    "        store output of forward computation is self.output attribute\n",
    "        '''\n",
    "        self.input_ = input_\n",
    "        self.output = self._output()\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Calls the self._input_grad() function\n",
    "        '''\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        assert(self.input_grad.shape == self.input_.shape)\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self) -> np.ndarray:\n",
    "        '''\n",
    "        the output method must be defined for each operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _input_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        the input_grad method must be defined for each operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a class for operation that also involves parameters - for example matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamOperation(Operation):\n",
    "    def __init__(self,param : np.ndarray) -> np.ndarray:\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "    \n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        assert(self.input_grad.shape == self.input_.shape)\n",
    "        assert(self.param_grad.shape == self.param.shape)\n",
    "\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _param_grad(self,output_grad : np.ndarray) -> np.ndarray :\n",
    "        raise NotImplementedError()\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learing model : Building blocks on top of our base class \\\n",
    "for the neural network in the previous chapter, we need three different operations per layer : \n",
    "1. Matrix multiplication\n",
    "2. Addition of a bias term\n",
    "3. Non-linear activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weightMultiply(ParamOperation):\n",
    "    def __init__(self, W : np.ndarray):\n",
    "        super().__init__(W)\n",
    "    \n",
    "    def _output(self) -> np.ndarray :\n",
    "        return np.dot(self.input_, self.param)\n",
    "        \n",
    "    def _input_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1,0)))\n",
    "    \n",
    "    def _param_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        return np.dot(np.transpose(self.input_,(1,0)), output_grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Addition of bias term\n",
    "class BiasAdd(ParamOperation):\n",
    "    def __init__(self,B:np.ndarray):\n",
    "        assert B.shape[0] == 1\n",
    "        super().__init__(B)\n",
    "    \n",
    "    def _output(self):\n",
    "        return self.input_ + self.param\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        return np.ones_like(self.input_) *  output_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, self.param.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid activation layer\n",
    "\n",
    "class Sigmoid(Operation):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self) -> np.ndarray:\n",
    "        return (1.0/(1.0+np.exp(-1.0 * self.input_)))\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        sigmoid_backward = self.output * (1-self.output) # derivative of sigmoid(x) = sigmoid(x) * (1-sigmoid(x))\n",
    "        return (sigmoid_backward * output_grad)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the above operations constitute a layer -> we could now write a Layer class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract layer class\n",
    "class Layer(object):\n",
    "    def __init__(self, neurons : int) :\n",
    "        self.first = True # first layer or not\n",
    "        self.neurons = neurons\n",
    "        self.params : List[np.ndarray] = []\n",
    "        self.param_grads : List[np.ndarray] = []\n",
    "        self.operations : List[Operation] = []\n",
    "    \n",
    "    def _setup_layer(self, input_ : np.ndarray) -> None:\n",
    "        raise NotImplementedError() # to be filled in derived class\n",
    "    \n",
    "    def forward(self, input_ : np.ndarray) -> np.ndarray : \n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "        self.input_ = input_\n",
    "        for operation in self.operations: \n",
    "            input_ = operation.forward(input_)\n",
    "        \n",
    "        self.output = input_\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "        \n",
    "        input_grad = output_grad\n",
    "        self._param_grads()\n",
    "        return input_grad\n",
    "    \n",
    "    def _param_grads(self):\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "            \n",
    "    def _params(self):\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation):\n",
    "                self.params.append(operation.param)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have the base class Layer and multiple layers with different functionalities can be subclassed from this class . example is the dense layer class, which is just matrix multiplication of input neurons with output neurons. Let us start writing the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, neurons : int, activation : Operation = Sigmoid()) -> None:\n",
    "        super().__init__(neurons)\n",
    "        self.activation = activation\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
