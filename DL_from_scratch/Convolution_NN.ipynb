{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolution neural networks : Densely neural networks learn relationships between data to predict output correctly. In images, we have some additional properties which can help us design neural networks in a different better way. for example, there are spatial relations like a 3X3 patch of adjacent pixels provide more information than 9 randomly selected pixels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage of 3X3 or 5X5 filters as pattern detectors is a well known idea in computer vision. For example we could use a filter with negative value in middle and positive values in the sides to detect edges and similar ones for corners, horizontal lines etc. by taking these filters and convolving with the input image, we can determine which patch of image is sensitive to these filters and thus has corners, edges etc. the output of this convolution operation is usually called the feature map. \n",
    "\n",
    "In deep learning, we let the model determine these filters based on the objective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in convolutional neural networks, we have f sets of such features per pixel. So, if we have n pixels as input, if we have 3X3 weight matrix and convolve , we have n feature maps [one per image patch in input]. we also have f such 3x3 [or 5X5] weight matrices called convolutional filters. and number of such filters is usually called channels --> Multichannel convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have these feature maps, to finally have the class probabilities for let us say digits, we use the flatten layer to squash the channels together and have a matrix multiplication with out dimension as num_classes and use the softmax cross entropy loss as discussed in previous chapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling : we don't the entire image dimension at each layer of CNN. instead , we downsize the feature map dimension and increase the number of filters at each layer. this is done using pooling layers. \\\n",
    "pooling takes a bunch of pixels like 2X2 patch and finds the max or average of the values and puts it in the output. so, the feature map size reduces from nxn to n/2 x n/2 - a 4 times reduction. \\\n",
    "the main advantage of pooling is computational . it reduces the number of training parameters thereby reducing the memory footprint of the model. Also, fewer computations are needed and this is compounded as number of layers increase. \\\n",
    "But, the model loses 1/4 the information at each layer . the performance on benchmarks is reasonably good and thus it was used before in many models due to computational efficiency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maintain the output shape of convolution of input with filter to be the same as input shape, we need to pad the input of size filter_size/2. \\\n",
    "We will take a simple 1D case and code the convolution up and scale that to bigger dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1D = np.array([1,2,3,4,5])\n",
    "param1D = np.array([1,1,1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_1D(inp : np.ndarray, num : int) -> np.ndarray :\n",
    "    z = np.array([0])\n",
    "    z = np.repeat(z,num)\n",
    "    return np.concatenate((z,inp,z))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1D_padded = _pad_1D(input1D, param1D.shape[0]/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1D_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d(inp : np.ndarray, param : np.ndarray) -> np.ndarray :\n",
    "    assert(len(inp.shape) == 1)\n",
    "    assert(len(param.shape) == 1) # for 1d conv\n",
    "\n",
    "    out = np.zeros(shape=inp.shape)\n",
    "\n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2\n",
    "    inp_pad = _pad_1D(inp, param_mid)\n",
    "\n",
    "    for o in range(out.shape[0]):\n",
    "        for p in range(param_len):\n",
    "            out[o] += (param[p] * inp_pad[o+p])\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  6.,  9., 12.,  9.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d(input1D, param1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a basic convolution has been coded up! there is another hyperparamter. Pooling is one way of scaling down the feature map size. it loses 1/4 th the amout of information for 2X2 pooling. there is also another way of reducing the computational load without pooling. it is by increasing the stride. it is the jump in sliding window each time convolution happens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "backpropagation of gradients is slightly tricky for convolution. we will find it out for 1d case and then scale it up to 2d and more channels case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_sum(inp : np.ndarray, param : np.ndarray) -> np.ndarray :\n",
    "    out = conv1d(inp, param)\n",
    "    return np.sum(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a pattern to calculate the gradients. assuming $\\frac{\\partial{(L)}}{\\partial{(o_i)}}$ is available, we see that : \n",
    "$\\frac{\\partial{(L)}}{\\partial{(i_i)}} = \\frac{\\partial{(L)}}{\\partial{(o_j)}} \\times \\frac{\\partial{(o_j)}}{\\partial{(i_i)}}$ for j = 1...n [in our case 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this boils down to case, partial derivate of L with respect to i is partial derivative of L with respect to o multiplied by w, with weight indices increasing and partial derivative of o decreasing. To code this up in loop, we introduce padding to output grad -> this because sometimes there are cases where we need to multiply only two of the three weights especially for $i_1$ and $i_5$ corner ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_grad_1D(output_grad : np.ndarray, inp : np.ndarray, param : np.ndarray) -> np.ndarray : \n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2\n",
    "    inp_pad = _pad_1D(inp, param_mid)\n",
    "\n",
    "    if output_grad is None:\n",
    "        output_grad = np.ones_like(inp)\n",
    "    else:\n",
    "        assert(inp.shape == output_grad.shape)\n",
    "\n",
    "    output_grad_pad = _pad_1D(output_grad)\n",
    "\n",
    "    input_grad = np.zeros_like(inp)\n",
    "\n",
    "    for o in range(output_grad.shape[0]):\n",
    "        for p in range(param.shape[0]):\n",
    "            input_grad[o] += param[p] * output_grad_pad[o+param_len - p - 1]\n",
    "    \n",
    "    return input_grad\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grad_1D(output_grad : np.ndarray, param : np.ndarray, inp : np.ndarray) -> np.ndarray:\n",
    "    assert(output_grad.shape == inp.shape)\n",
    "    param_len = param.shape[0]\n",
    "    param_mid = param_len // 2\n",
    "\n",
    "    param_grad = np.zeros_like(param)\n",
    "    input_pad = _pad_1D(inp)\n",
    "\n",
    "    for o in range(output_grad.shape[0]):\n",
    "        for p in range(param_len):\n",
    "            param_grad[p] += input_pad[o+p] * output_grad[o]\n",
    "    \n",
    "    return param_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding a layer of complexity : batches \\\n",
    "now we have input of shape (batches, input) -> 2D numpy array. need to do convolution for these inputs and find gradients. \\\n",
    "Example : input = np.array([[0,1,2,3,4,],[5,6,7,8,9]]) 2X5 array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_1d_batch(inp : np.ndarray, num : int) -> np.ndarray : \n",
    "    outs = [_pad_1D(obs,num) for obs in inp]\n",
    "    return np.stack(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_batch(inp : np.ndarray, param : np.ndarray) -> np.ndarray : \n",
    "    outs = [conv1d(obs,param) for obs in inp]\n",
    "    return np.stack(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_1d_batch = np.array([[1,2,3,4,5],[6,7,8,9,10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_padded = _pad_1d_batch(input_1d_batch, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.,  6.,  9., 12.,  9.],\n",
       "       [13., 21., 24., 27., 19.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1d_batch(input_1d_batch, param1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropagation for input with batches\n",
    "\n",
    "def input_grad_1D_batch(output_grad : np.ndarray, inp : np.ndarray, param : np.ndarray) -> np.ndarray : \n",
    "    batch_size = output_grad.shape[0]\n",
    "    grads = [input_grad_1D(output_grad[i], inp[i], param) for i in range(batch_size)]\n",
    "    return np.stack(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_grad_1D_batch(output_grad : np.ndarray, inp : np.ndarray, param : np.ndarray) -> np.ndarray : \n",
    "    param_grad = np.zeros_like(param)\n",
    "    assert(inp.shape == output_grad.shape)\n",
    "    inp_pad = _pad_1d_batch(inp)\n",
    "    for i in range(inp.shape[0]):\n",
    "        for o in range(inp.shape[1]):\n",
    "            for p in range(param.shape):\n",
    "                param_grad[p] += inp_pad[i][o+p] * output_grad[i][o]\n",
    "    \n",
    "    return param_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next step is to move on to 2D convolutions from 1D convolutions. we are not processing batches for now. that will be added after 2D convolution discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_2d = np.random.randn(3,28,28) # three channel img like RGB\n",
    "\n",
    "param_2d = np.random.randn(3,3) # 3X3 filter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_2D_obs(inp : np.ndarray, num : int) -> np.ndarray : \n",
    "   inp_pad = _pad_1d_batch(inp,num)\n",
    "   border = np.zeros((num,inp.shape[0]+num*2))\n",
    "   return np.concatenate([border, inp_pad, border])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_2D(inp : np.ndarray, num : int) -> np.ndarray : \n",
    "    z = [_pad_2D_obs(obs, num) for obs in inp]\n",
    "    return np.stack(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 30, 30)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_pad_2D(imgs_2d, 1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward pass for 2d convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_2d_output_obs(inp : np.ndarray, param : np.ndarray) -> np.ndarray:\n",
    "    param_mid = param.shape[0] // 2\n",
    "    inp_pad = _pad_2D_obs(inp, param_mid)\n",
    "    out = np.zeros_like(inp)\n",
    "    for o_h in range(inp.shape[0]):\n",
    "        for o_w in range(inp.shape[1]):\n",
    "            for p_h in range(param.shape[0]):\n",
    "                for p_w in range(param.shape[1]):\n",
    "                    out[o_h][o_w] += inp_pad[o_h + p_h][o_w + p_w] * param[p_h][p_w]\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_2d_output(inp : np.ndarray , param : np.ndarray) : \n",
    "    outs = [compute_2d_output_obs(img,param) for img in inp]\n",
    "    return np.stack(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 28, 28)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_2d_output(imgs_2d, param_2d).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_grads_2d(output_grad : np.ndarray, inp: np.ndarray, param : np.ndarray) -> np.ndarray:\n",
    "    outs = []\n",
    "    for i in range(output_grad.shape[0]):\n",
    "        param_size = param.shape[0]\n",
    "        output_grad_obs = _pad_2D_obs(output_grad[i], param_size//2)\n",
    "        input_grads = np.zeros_like(inp[i])\n",
    "        for i_h in range(inp[i].shape[0]):\n",
    "            for i_w in range(inp[i].shape[1]):\n",
    "                for p_h in range(param.shape[0]):\n",
    "                    for p_w in range(param.shape[1]):\n",
    "                        input_grads[i_h][i_w] += param[p_h][p_w] * output_grad_obs[i_h + param_size - p_h-1][i_w + param_size - p_w - 1]\n",
    "        outs.append(input_grads)\n",
    "    return np.stack(outs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_2d_grad(output_grad : np.ndarray, param : np.ndarray, inp : np.ndarray) -> np.ndarray:\n",
    "    param_grad = np.zeros_like(param)\n",
    "\n",
    "    channels = inp.shape[0]\n",
    "    param_size = param.shape[0]\n",
    "    input_pad = _pad_2D(inp, param_size//2)\n",
    "    for i in range(channels):\n",
    "        for i_h in range(inp.shape[1]):\n",
    "            for i_w in range(inp.shape[2]):\n",
    "                for p_h in range(param.shape[0]):\n",
    "                    for p_w in range(param.shape[1]):\n",
    "                        param_grad[p_h][p_w] += output_grad[i][i_h][i_w] * input_pad[i][i_h+p_h][i_w+p_w]\n",
    "    \n",
    "    return param_grad\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 28, 28)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify shapes\n",
    "input_2d_grad = input_grads_2d(np.ones_like(imgs_2d), imgs_2d, param_2d)\n",
    "input_2d_grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.81081701, -5.34547063, -5.34547063, -5.34547063, -5.34547063])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2d_grad.flatten()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grad_2d = param_2d_grad(np.ones_like(imgs_2d), param_2d, imgs_2d)\n",
    "param_grad_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -4.96823183, -13.83603079, -16.73461795],\n",
       "       [ -2.86856529, -13.73812317, -18.16716972],\n",
       "       [-18.72853249, -29.22406312, -33.57882624]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grad_2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is one final step as far as convolution operation is concerned -> adding batches to 2d and channels [till now we had one feature map], now multiple feature maps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input shape : batch_size, channel, height, width\n",
    "# param shape : in_channel, out_channel, filter_height, filter_width\n",
    "\n",
    "def _pad_2d_channel(inp : np.ndarray , num : int) -> np.ndarray :\n",
    "    outs = [_pad_2D_obs(channel,num) for channel in inp]\n",
    "    return np.stack(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_conv_input(inp : np.ndarray, num : int) -> np.ndarray : \n",
    "    outs = [_pad_2d_channel(obs,num) for obs in inp]\n",
    "    return np.stack(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward pass\n",
    "def compute_output_obs(inp : np.ndarray, param : np.ndarray) -> np.ndarray :\n",
    "    assert(len(inp.shape) == 3)\n",
    "    assert(len(param.shape) == 4) \n",
    "    param_size = param.shape[2]\n",
    "    param_mid = param_size // 2\n",
    "    obs_pad = _pad_2d_channel(inp,param_mid)\n",
    "\n",
    "    in_channels = param.shape[0]\n",
    "    out_channels = param.shape[1]\n",
    "\n",
    "    img_size = inp.shape[1]\n",
    "\n",
    "    out = np.zeros((out_channels, inp.shape[1],inp.shape[2]))\n",
    "\n",
    "    for c_out in range(out_channels):\n",
    "        for c_in in range(in_channels):\n",
    "            for i_h in range(inp.shape[1]):\n",
    "                for i_w in range(inp.shape[2]):\n",
    "                    for p_h in range(param.shape[2]):\n",
    "                        for p_w in range(param.shape[3]):\n",
    "                            out[c_out][i_h][i_w] += obs_pad[c_in][i_h + p_h][i_w + p_w] * param[c_in][c_out][p_h][p_w]\n",
    "    return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_output(inp : np.ndarray, param : np.ndarray) -> np.ndarray :\n",
    "    outs = [compute_output_obs(obs, param) for obs in inp]\n",
    "    return np.stack(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_input_grads(input_obs : np.ndarray, output_grad_obs : np.ndarray, param : np.ndarray) -> np.ndarray : \n",
    "    input_grad = np.zeros_like(input_obs)\n",
    "    param_size = param.shape[2]\n",
    "    param_mid = param_size // 2\n",
    "    img_size = input_obs.shape[1]\n",
    "    in_channels = input_obs.shape[0]\n",
    "    out_channels = param.shape[1]\n",
    "    output_obs_pad = _pad_2d_channel(output_grad_obs, param_mid)\n",
    "\n",
    "    for c_in in range(in_channels):\n",
    "        for c_out in range(out_channels):\n",
    "            for i_h in range(input_obs.shape[1]):\n",
    "                for i_w in range(input_obs.shape[2]):\n",
    "                    for p_h in range(param.shape[2]):\n",
    "                        for p_w in range(param.shape[3]):\n",
    "                            input_grad[c_in][i_h][i_w] += output_obs_pad[c_out][i_h + param_size - p_h-1][i_w + param_size - p_w - 1] * param[c_in][p_h][p_w]\n",
    "    return input_grad\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backward pass\n",
    "\n",
    "def _input_grad(output_grad : np.ndarray, inp : np.ndarray, param : np.ndarray) -> np.ndarray :\n",
    "    grads = [_compute_input_grads(inp[i], output_grad[i],param) for i in range(output_grad.shape[0])]\n",
    "    return np.stack(grads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _param_grad(inp : np.ndarray, output_grad : np.ndarray, param : np.ndarray) :\n",
    "    param_grad = np.zeros_like(param)\n",
    "    param_size = param.shape[2]\n",
    "    param_mid = param_size // 2\n",
    "    img_size = inp.shape[2]\n",
    "    in_channels = inp.shape[1]\n",
    "    out_channels = output_grad.shape[1]\n",
    "\n",
    "    inp_pad = _pad_conv_input(inp, param_mid)\n",
    "    img_shape = output_grad.shape[2:]\n",
    "\n",
    "    for i in range(inp.shape[0]):\n",
    "        for c_in in range(in_channels):\n",
    "            for c_out in range(out_channels):\n",
    "                for o_h in range(img_shape[0]):\n",
    "                    for o_w in range(img_shape[1]):\n",
    "                        for p_h in range(param_size):\n",
    "                            for p_w in range(param_size):\n",
    "                                param_grad[c_in][c_out][p_h][p_w] += output_grad[i][c_out][o_h][o_w] * inp_pad[i][c_in][o_h + p_h][o_w + p_w]\n",
    "    \n",
    "    return param_grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten layer needs to be written. to use these convolution operations and train a model, having for loops is inefficient.instead we have to write vectorized numpy code. We will write a conv2d class inheriting from Layer class from the previous notebook.Also , we will use other classes like Operation, Neural Network, Trainer etc from that notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Tuple, Dict\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    '''\n",
    "    Loss function of the neural network\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, prediction : np.ndarray, target : np.ndarray) -> float :\n",
    "        assert(prediction.shape == target.shape)\n",
    "        self.prediction = prediction\n",
    "        self.target = target\n",
    "        loss_value = self._output()\n",
    "        return loss_value\n",
    "    \n",
    "    def backward(self) -> np.ndarray :\n",
    "        self.input_grad = self._input_grad()\n",
    "        assert(self.input_grad.shape == self.prediction.shape)\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self) -> float:\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _input_grad(self) -> np.ndarray:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "# Mean squared loss : a subclass of Loss class\n",
    "\n",
    "class MeanSquaredLoss(Loss):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self) -> float:\n",
    "        return np.sum(np.power((self.prediction-self.target),2)) / self.prediction.shape[0]\n",
    "    \n",
    "    def _input_grad(self) -> np.ndarray:\n",
    "        return 2.0 * (self.prediction-self.target) / self.prediction.shape[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next step is to implement learning rate decay\n",
    "# this is incorporated inside the optimizer\n",
    "class Optimizer2(object):\n",
    "    def __init__(self, initial_lr : float = 0.01, final_lr : float = 0., decay_type : str = \"exponential\"):\n",
    "        self.lr = initial_lr\n",
    "        self.final_lr = final_lr\n",
    "        self.decay_type = decay_type\n",
    "        self.first = True\n",
    "    def step(self):\n",
    "        pass\n",
    "\n",
    "    def _setup_decay(self) -> None:\n",
    "        if not self.decay_type:\n",
    "            return\n",
    "        elif self.decay_type == \"exponential\":\n",
    "            self.decay_per_epoch = np.power(self.final_lr/self.lr, 1.0/(self.max_epochs-1))\n",
    "        \n",
    "        elif self.decay_type == \"linear\":\n",
    "            self.decay_per_epoch = (self.lr - self.final_lr) / (self.max_epochs - 1)\n",
    "    \n",
    "    def _decay_lr(self) -> None : \n",
    "        if not self.decay_type:\n",
    "            return\n",
    "        elif self.decay_type == \"exponential\":\n",
    "            self.lr *= self.decay_per_epoch\n",
    "        elif self.decay_type == \"linear\":\n",
    "            self.lr -= self.decay_per_epoch\n",
    "        \n",
    "\n",
    "class SGD2(Optimizer2):\n",
    "    def __init__(self, lr : float = 0.001, final_lr : float = 0., decay_type : str = \"exponential\"):\n",
    "        super().__init__(lr, final_lr, decay_type)\n",
    "\n",
    "    \n",
    "    def step(self) -> None:\n",
    "        for (param,param_grad) in zip(self.net.params(), self.net.param_grads()):\n",
    "            param -= self.lr * param_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum2(Optimizer2):\n",
    "    def __init__(\n",
    "        self, lr: float = 0.01, final_lr : float = 0. , decay_type : str = \"exponential\", momentum: float = 0.9\n",
    "    ) -> None:\n",
    "        super().__init__(lr, final_lr,decay_type)\n",
    "        self.momentum = momentum\n",
    "\n",
    "    def step(self) -> None:\n",
    "        if self.first:\n",
    "            self.velocities = [np.zeros_like(param) for param in self.net.params()]\n",
    "            self.first = False\n",
    "\n",
    "        for param, param_grad, velocity in zip(\n",
    "            self.net.params(), self.net.param_grads(), self.velocities\n",
    "        ):\n",
    "            self._update_rule(param=param, grad=param_grad, velocity=velocity)\n",
    "\n",
    "    def _update_rule(self, **kwargs) -> None:\n",
    "\n",
    "        # Update velocity\n",
    "        kwargs[\"velocity\"] *= self.momentum\n",
    "        kwargs[\"velocity\"] += self.lr * kwargs[\"grad\"]\n",
    "\n",
    "        # Use this to update parameters\n",
    "        kwargs[\"param\"] -= kwargs[\"velocity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper functions\n",
    "def permute_data(X, y):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    return X[perm], y[perm]\n",
    "\n",
    "def mae(preds: np.ndarray, actuals: np.ndarray):\n",
    "    '''\n",
    "    Compute mean absolute error.\n",
    "    '''\n",
    "    return np.mean(np.abs(preds - actuals))\n",
    "\n",
    "def rmse(preds: np.ndarray, actuals: np.ndarray):\n",
    "    '''\n",
    "    Compute root mean squared error.\n",
    "    '''\n",
    "    return np.sqrt(np.mean(np.power(preds - actuals, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation_final(object):\n",
    "    '''\n",
    "    Base class for an operation in a neural network\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input_ : np.ndarray, inference : bool = False):\n",
    "        \n",
    "        '''\n",
    "        Stores input in the self.input attribute. \n",
    "        store output of forward computation is self.output attribute\n",
    "        '''\n",
    "        self.input_ = input_\n",
    "\n",
    "        self.output = self._output(inference)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Calls the self._input_grad() function\n",
    "        '''\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        assert(self.input_grad.shape == self.input_.shape)\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray:\n",
    "        '''\n",
    "        the output method must be defined for each operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def _input_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        the input_grad method must be defined for each operation\n",
    "        '''\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "class ParamOperation_final(Operation_final):\n",
    "    def __init__(self,param : np.ndarray) -> np.ndarray:\n",
    "        super().__init__()\n",
    "        self.param = param\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "    \n",
    "        self.input_grad = self._input_grad(output_grad)\n",
    "        self.param_grad = self._param_grad(output_grad)\n",
    "        assert(self.input_grad.shape == self.input_.shape)\n",
    "        assert(self.param_grad.shape == self.param.shape)\n",
    "\n",
    "        return self.input_grad\n",
    "    \n",
    "    def _param_grad(self,output_grad : np.ndarray) -> np.ndarray :\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "class weightMultiply_final(ParamOperation_final):\n",
    "    def __init__(self, W : np.ndarray):\n",
    "        super().__init__(W)\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray :\n",
    "        return np.dot(self.input_, self.param)\n",
    "        \n",
    "    def _input_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        return np.dot(output_grad, np.transpose(self.param, (1,0)))\n",
    "    \n",
    "    def _param_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        return np.dot(np.transpose(self.input_,(1,0)), output_grad)\n",
    "\n",
    "# Addition of bias term\n",
    "class BiasAdd_final(ParamOperation_final):\n",
    "    def __init__(self,B:np.ndarray):\n",
    "        assert B.shape[0] == 1\n",
    "        super().__init__(B)\n",
    "    \n",
    "    def _output(self, inference : bool = False):\n",
    "        return self.input_ + self.param\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        return np.ones_like(self.input_) *  output_grad\n",
    "    \n",
    "    def _param_grad(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        param_grad = np.ones_like(self.param) * output_grad\n",
    "        return np.sum(param_grad, axis=0).reshape(1, self.param.shape[1])\n",
    "\n",
    "# sigmoid activation layer\n",
    "\n",
    "class Sigmoid_final(Operation_final):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray:\n",
    "        return (1.0/(1.0+np.exp(-1.0 * self.input_)))\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        sigmoid_backward = self.output * (1-self.output) # derivative of sigmoid(x) = sigmoid(x) * (1-sigmoid(x))\n",
    "        return (sigmoid_backward * output_grad)\n",
    "\n",
    "class Linear_final(Operation_final):\n",
    "    def __init__(self) -> None :\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray:\n",
    "        return self.input_\n",
    "    \n",
    "    def _input_grad(self, output_grad:np.ndarray) -> np.ndarray:\n",
    "        return output_grad\n",
    "    \n",
    "class Tanh_final(Operation_final):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray:\n",
    "        return np.tanh(self.input_)\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        tanh_backward = 1 - self.output * self.output # derivative of tanh(x) = (1-tanh(x)^2)\n",
    "        return (tanh_backward * output_grad)\n",
    "    \n",
    "class Dropout(Operation_final):\n",
    "    def __init__(self, keep_prob : float = 0.8):\n",
    "        super().__init__()\n",
    "        self.keep_prob = keep_prob\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray:\n",
    "        if inference : \n",
    "            return self.input_ * self.keep_prob\n",
    "        else:\n",
    "            self.mask = np.random.binomial(1, self.keep_prob, size=self.input_.shape)\n",
    "            return self.input_ * self.mask \n",
    "    \n",
    "    def _input_grad(self,output_grad : np.ndarray) -> np.ndarray:\n",
    "        return output_grad * self.mask\n",
    "    \n",
    "# abstract layer class\n",
    "class Layer_final(object):\n",
    "    def __init__(self, neurons : int, dropout : float = 1.) :\n",
    "        self.first = True # first layer or not\n",
    "        self.neurons = neurons\n",
    "        self.params : List[np.ndarray] = []\n",
    "        self.param_grads : List[np.ndarray] = []\n",
    "        self.operations : List[Operation_final] = []\n",
    "        self.dropout = dropout\n",
    "\n",
    "\n",
    "    \n",
    "    def _setup_layer(self, input_ : np.ndarray) -> None:\n",
    "        raise NotImplementedError() # to be filled in derived class\n",
    "    \n",
    "    def forward(self, input_ : np.ndarray, inference : bool = False) -> np.ndarray : \n",
    "        if self.first:\n",
    "            self._setup_layer(input_)\n",
    "            self.first = False\n",
    "            \n",
    "        self.input_ = input_\n",
    "        \n",
    "        for operation in self.operations: \n",
    "            \n",
    "            input_ = operation.forward(input_, inference)\n",
    "        \n",
    "        self.output = input_\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        assert(self.output.shape == output_grad.shape)\n",
    "\n",
    "        for operation in reversed(self.operations):\n",
    "            output_grad = operation.backward(output_grad)\n",
    "        \n",
    "        input_grad = output_grad\n",
    "        self._param_grads()\n",
    "        return input_grad\n",
    "    \n",
    "    def _param_grads(self):\n",
    "        self.param_grads = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation_final):\n",
    "                self.param_grads.append(operation.param_grad)\n",
    "            \n",
    "    def _params(self):\n",
    "        self.params = []\n",
    "        for operation in self.operations:\n",
    "            if issubclass(operation.__class__, ParamOperation_final):\n",
    "                self.params.append(operation.param)\n",
    "\n",
    "class Dense_final(Layer_final):\n",
    "    def __init__(self, neurons : int, activation : Operation_final = Sigmoid_final(), weight_init = \"glorot\", dropout : float = 1.) -> None:\n",
    "        super().__init__(neurons, dropout)\n",
    "        self.activation = activation\n",
    "        self.weight_init = weight_init\n",
    "\n",
    "\n",
    "    \n",
    "    def _setup_layer(self, input_:np.ndarray) -> None:\n",
    "        if self.seed:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        if self.weight_init == \"glorot\":\n",
    "            scale = 2 / (input_.shape[1] + self.neurons)\n",
    "        else:\n",
    "            scale = 1.0\n",
    "        \n",
    "        self.params = []\n",
    "\n",
    "        self.params.append(scale * np.random.randn(input_.shape[1],self.neurons))\n",
    "        self.params.append(np.random.randn(1,self.neurons))\n",
    "        self.operations = [weightMultiply_final(self.params[0]),\n",
    "                           BiasAdd_final(self.params[1]),\n",
    "                           self.activation]\n",
    "        if self.dropout < 1.:\n",
    "            self.operations.append(Dropout(self.dropout))\n",
    "\n",
    "        \n",
    "        return None \n",
    "    \n",
    "# neural network class\n",
    "class NeuralNetwork_final(object):\n",
    "   def __init__(self,layers : List[Layer_final],\n",
    "                loss : Loss, seed : float = 1):\n",
    "      self.layers = layers\n",
    "      self.loss = loss\n",
    "      self.seed = seed\n",
    "      for layer in self.layers:\n",
    "         setattr(layer, \"seed\", self.seed)\n",
    "    \n",
    "   def forward(self, x_batch : np.ndarray, inference : bool = False) -> np.ndarray:\n",
    "      x_out = x_batch\n",
    "\n",
    "      for layer in self.layers:\n",
    "         x_out = layer.forward(x_out,inference)\n",
    "      \n",
    "      return x_out\n",
    "\n",
    "   def backward(self, loss_grad : np.ndarray) -> None:\n",
    "      grad = loss_grad\n",
    "      for layer in reversed(self.layers):\n",
    "         grad = layer.backward(grad)\n",
    "   \n",
    "   def train_batch(self, X_batch : np.ndarray, y_batch : np.ndarray) -> float:\n",
    "      predictions = self.forward(X_batch)\n",
    "      loss = self.loss.forward(predictions, y_batch)\n",
    "      loss_grad = self.loss.backward()\n",
    "      self.backward(loss_grad)\n",
    "      return loss\n",
    "   \n",
    "   def params(self):\n",
    "      for layer in self.layers:\n",
    "         yield from layer.params # get the parameters from the generator instead of writing a loop and getting the params for the batch\n",
    "\n",
    "   def param_grads(self):\n",
    "      for layer in self.layers:\n",
    "         yield from layer.param_grads\n",
    "         \n",
    "class Trainer_final(object):\n",
    "    def __init__(self, net : NeuralNetwork_final, optim : Optimizer2) -> None:\n",
    "        self.net = net\n",
    "        self.optim = optim\n",
    "        self.best_loss = np.inf # initial value for best loss\n",
    "        setattr(self.optim, \"net\",self.net)\n",
    "\n",
    "    \n",
    "    def generate_batches(self,X:np.ndarray, y:np.ndarray, batch_size : int = 32) -> Tuple[np.ndarray] :\n",
    "        assert X.shape[0] == y.shape[0] # shape check\n",
    "        N = X.shape[0]\n",
    "        for i in range(0,N,batch_size):\n",
    "            X_batch, y_batch = X[i:i+batch_size], y[i:i+batch_size]\n",
    "            yield X_batch, y_batch\n",
    "    \n",
    "    def fit(self, X_train : np.ndarray, y_train : np.ndarray, \n",
    "            X_test : np.ndarray, y_test : np.ndarray, batch_size : int = 32,\n",
    "            epochs : int = 100, eval_every : int = 100, seed : int = 42, restart : bool  = True) -> None:\n",
    "        np.random.seed(seed)\n",
    "        setattr(self.optim, \"max_epochs\", epochs)\n",
    "        self.optim._setup_decay()\n",
    "        if restart:\n",
    "            for layer in self.net.layers:\n",
    "                layer.first = True\n",
    "            \n",
    "            self.best_loss = np.inf\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            if (i+1)%eval_every == 0:\n",
    "                last_model = deepcopy(self.net)\n",
    "            \n",
    "            X_train, y_train = permute_data(X_train, y_train)\n",
    "            batch_generator = self.generate_batches(X_train, y_train)\n",
    "            for ii, (X_batch, y_batch) in enumerate(batch_generator):\n",
    "                self.net.train_batch(X_batch, y_batch)\n",
    "                self.optim.step()\n",
    "            if self.optim.final_lr:\n",
    "                self.optim._decay_lr()\n",
    "            \n",
    "            if (i+1)%eval_every == 0:\n",
    "                test_preds = self.net.forward(X_test, inference = True)\n",
    "                loss = self.net.loss.forward(test_preds, y_test)\n",
    "\n",
    "                if loss < self.best_loss:\n",
    "                    print(f\"Validation loss after {i+1} epochs is {loss:.3f}\")\n",
    "                    self.best_loss = loss\n",
    "                else:\n",
    "                    self.net = last_model\n",
    "                    print(f\"\"\"Loss increased after epoch {i+1}, final loss was {self.best_loss:.3f}, using the model from epoch {i+1-eval_every}\"\"\")\n",
    "                    setattr(self.optim, \"net\", self.net)\n",
    "                    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution Layer class\n",
    "class conv2D(Layer_final):\n",
    "    def __init__(self,out_channels : int, param_size : int, \n",
    "                 dropout : int = 1., weight_init : str = \"glorot\", \n",
    "                 activation : Operation_final = Linear_final(), flatten : bool = False):\n",
    "        super().__init__(out_channels, dropout)\n",
    "        self.param_size = param_size\n",
    "        self.activation = activation\n",
    "        self.flatten = flatten\n",
    "        self.weight_init = weight_init\n",
    "        self.out_channels = out_channels\n",
    "    \n",
    "    def _setup_layer(self,input : np.ndarray) -> None :\n",
    "        in_channels = input.shape[1]\n",
    "        if self.weight_init:\n",
    "            scale = 2 / (in_channels + self.out_channels)\n",
    "        else:\n",
    "            scale = 1\n",
    "        \n",
    "        conv_param = np.random.normal(\n",
    "            loc = 0, scale = scale, size = (in_channels, self.out_channels, self.param_size, self.param_size )\n",
    "\n",
    "        )\n",
    "        self.params.append(conv_param)\n",
    "        self.operations = []\n",
    "        self.operations.append(Conv2d_op(conv_param))\n",
    "        self.operations.append(self.activation)\n",
    "\n",
    "        if self.flatten:\n",
    "            self.operations.append(Flatten())\n",
    "        \n",
    "        if self.dropout < 1. :\n",
    "            self.operations.append(Dropout(self.dropout))\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Operation_final):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def _output(self, inference : bool = False) -> np.ndarray:\n",
    "        return self.input_.reshape(self.input_.shape[0],-1)\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray : \n",
    "        return output_grad.reshape(self.input_.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import special\n",
    "def unnormalize(a : np.ndarray):\n",
    "    return a[np.newaxis,0]\n",
    "\n",
    "def normalize(a : np.ndarray) -> np.ndarray :\n",
    "    other = 1-a\n",
    "    return np.concatenate([a,1-a], axis=1)\n",
    "\n",
    "def softmax(x: np.ndarray, axis=None) -> np.ndarray:\n",
    "    return np.exp(x - special.logsumexp(x, axis=axis, keepdims=True))\n",
    "# Softmax cross entropy class\n",
    "class SoftmaxCrossEntropy(Loss):\n",
    "    def __init__(self, eps: float = 1e-9) -> None:\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.single_class = False\n",
    "\n",
    "    def _output(self) -> float:\n",
    "\n",
    "        # if the network is just outputting probabilities\n",
    "        # of just belonging to one class:\n",
    "        if self.target.shape[1] == 0:\n",
    "            self.single_class = True\n",
    "\n",
    "        # if \"single_class\", apply the \"normalize\" operation defined above:\n",
    "        if self.single_class:\n",
    "            self.prediction, self.target = normalize(self.prediction), normalize(\n",
    "                self.target\n",
    "            )\n",
    "\n",
    "        # applying the softmax function to each row (observation)\n",
    "        softmax_preds = softmax(self.prediction, axis=1)\n",
    "\n",
    "        # clipping the softmax output to prevent numeric instability\n",
    "        self.softmax_preds = np.clip(softmax_preds, self.eps, 1 - self.eps)\n",
    "\n",
    "        # actual loss computation\n",
    "        softmax_cross_entropy_loss = -1.0 * self.target * np.log(self.softmax_preds) - (\n",
    "            1.0 - self.target\n",
    "        ) * np.log(1 - self.softmax_preds)\n",
    "\n",
    "        return np.sum(softmax_cross_entropy_loss) / self.prediction.shape[0]\n",
    "\n",
    "    def _input_grad(self) -> np.ndarray:\n",
    "\n",
    "        # if \"single_class\", \"un-normalize\" probabilities before returning gradient:\n",
    "        if self.single_class:\n",
    "            return unnormalize(self.softmax_preds - self.target)\n",
    "        else:\n",
    "            return (self.softmax_preds - self.target) / self.prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_pad = 3//2\n",
    "param_size = 3\n",
    "def _pad_1d(inp: np.ndarray) -> np.ndarray:\n",
    "        z = np.array([0])\n",
    "        z = np.repeat(z, param_pad)\n",
    "        return np.concatenate([z, inp, z])\n",
    "\n",
    "def _pad_1d_batch(inp: np.ndarray) -> np.ndarray:\n",
    "        outs = [_pad_1d(obs) for obs in inp]\n",
    "        return np.stack(outs)\n",
    "\n",
    "def _pad_2d_obs(inp: np.ndarray):\n",
    "        \"\"\"\n",
    "        Input is a 2 dimensional, square, 2D Tensor\n",
    "        \"\"\"\n",
    "        inp_pad = _pad_1d_batch(inp)\n",
    "\n",
    "        other = np.zeros((param_pad, inp.shape[0] + param_pad * 2))\n",
    "\n",
    "        return np.concatenate([other, inp_pad, other])\n",
    "\n",
    "def _pad_2d_channel(inp: np.ndarray):\n",
    "        \"\"\"\n",
    "        inp has dimension [num_channels, image_width, image_height]\n",
    "        \"\"\"\n",
    "        return np.stack([_pad_2d_obs(channel) for channel in inp])\n",
    "\n",
    "def _get_image_patches(input_: np.ndarray):\n",
    "        imgs_batch_pad = np.stack([_pad_2d_channel(obs) for obs in input_])\n",
    "        patches = []\n",
    "        img_height = imgs_batch_pad.shape[2]\n",
    "        for h in range(img_height - param_size + 1):\n",
    "            for w in range(img_height - param_size + 1):\n",
    "                patch = imgs_batch_pad[:, :, h : h + param_size, w : w + param_size]\n",
    "                patches.append(patch)\n",
    "        return np.stack(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51984, 2, 3, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "input = np.random.randn(2,3,228,228)\n",
    "input_patches = _get_image_patches(input)\n",
    "print(input_patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51984"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 2\n",
    "img_size = input.shape[2]*input.shape[3]\n",
    "img_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_reshaped = input_patches.transpose(1, 0, 2, 3, 4).reshape(2, img_size, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 51984, 27)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d_op(ParamOperation_final):\n",
    "    def __init__(self, conv_param : np.ndarray):\n",
    "        super().__init__(conv_param)\n",
    "        self.param_size = conv_param.shape[2]\n",
    "        self.param_pad = self.param_size // 2\n",
    "    \n",
    "    def _pad_1d(self, inp: np.ndarray) -> np.ndarray:\n",
    "        z = np.array([0])\n",
    "        z = np.repeat(z, self.param_pad)\n",
    "        return np.concatenate([z, inp, z])\n",
    "\n",
    "    def _pad_1d_batch(self, inp: np.ndarray) -> np.ndarray:\n",
    "        outs = [self._pad_1d(obs) for obs in inp]\n",
    "        return np.stack(outs)\n",
    "\n",
    "    def _pad_2d_obs(self, inp: np.ndarray):\n",
    "        \"\"\"\n",
    "        Input is a 2 dimensional, square, 2D Tensor\n",
    "        \"\"\"\n",
    "        inp_pad = self._pad_1d_batch(inp)\n",
    "\n",
    "        other = np.zeros((self.param_pad, inp.shape[0] + self.param_pad * 2))\n",
    "\n",
    "        return np.concatenate([other, inp_pad, other])\n",
    "\n",
    "    def _pad_2d_channel(self, inp: np.ndarray):\n",
    "        \"\"\"\n",
    "        inp has dimension [num_channels, image_width, image_height]\n",
    "        \"\"\"\n",
    "        return np.stack([self._pad_2d_obs(channel) for channel in inp])\n",
    "\n",
    "    def _get_image_patches(self, input_: np.ndarray):\n",
    "        imgs_batch_pad = np.stack([self._pad_2d_channel(obs) for obs in input_])\n",
    "        patches = []\n",
    "        img_height = imgs_batch_pad.shape[2]\n",
    "        for h in range(img_height - self.param_size + 1):\n",
    "            for w in range(img_height - self.param_size + 1):\n",
    "                patch = imgs_batch_pad[:, :, h : h + self.param_size, w : w + self.param_size]\n",
    "                patches.append(patch)\n",
    "        return np.stack(patches)\n",
    "    \n",
    "    def _output(self, inference : bool = False):\n",
    "        batch_size = self.input_.shape[0]\n",
    "        img_height = self.input_.shape[2]\n",
    "        img_size = self.input_.shape[2] * self.input_.shape[3]\n",
    "        patch_size = self.param.shape[2] * self.param.shape[3] * self.param.shape[0]\n",
    "        patches = self._get_image_patches(self.input_) # num_patches X batch_size X in_channels X 3X3\n",
    "        patches_reshaped = patches.transpose(1,0,2,3,4).reshape(batch_size, img_size, -1) # batch_size X (hXW) X (in_channels X 3 X 3)\n",
    "        params_reshaped = self.param.transpose(0,2,3,1).reshape(patch_size,-1)\n",
    "        output_reshaped = np.matmul(patches_reshaped, params_reshaped).reshape(batch_size,img_height, img_height,-1).transpose(0,3,1,2)\n",
    "        return output_reshaped\n",
    "    \n",
    "    def _input_grad(self, output_grad : np.ndarray) -> np.ndarray:\n",
    "        batch_size = self.input_.shape[0]\n",
    "        img_size = self.input_.shape[2] * self.input_.shape[3]\n",
    "        img_height = self.input_.shape[2]\n",
    "\n",
    "        output_grad_pad = self._get_image_patches(output_grad).transpose(1,0,2,3,4).reshape(batch_size*img_size,-1) # 2 X (hXw) X (out_channel X 3 X 3)\n",
    "        param_reshaped = self.param.reshape(self.param.shape[0],-1).transpose(1,0) # (out_channel X 3 X3) X in_channel\n",
    "\n",
    "        input_grad = np.matmul(output_grad_pad, param_reshaped).reshape(batch_size, img_height, img_height, self.param.shape[0]).transpose(0,3,1,2)\n",
    "        return input_grad\n",
    "\n",
    "    def _param_grad(self, output_grad : np.ndarray) -> np.ndarray :\n",
    "        batch_size = self.input_.shape[0]\n",
    "        img_height = self.input_.shape[2]\n",
    "        img_size =  img_height * img_height \n",
    "        in_channel = self.param.shape[0]\n",
    "        out_channel = self.param.shape[1]\n",
    "\n",
    "        in_patches_reshape = self._get_image_patches(self.input_).transpose(1,0,2,3,4).reshape(batch_size*img_size,-1).transpose(1,0)\n",
    "        output_grad_reshape = output_grad.transpose(0,2,3,1).reshape(batch_size * img_size,-1)\n",
    "\n",
    "        param_grad = np.matmul(in_patches_reshape, output_grad_reshape).reshape(in_channel,self.param.shape[2], self.param.shape[3], out_channel).transpose(0,3,1,2)\n",
    "        return param_grad\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating the model\n",
    "# a singel convnet with dense layer containing 10 neurons\n",
    "\n",
    "model = NeuralNetwork_final(\n",
    "    layers=[conv2D(out_channels=16,\n",
    "                   param_size=5,\n",
    "                   dropout=0.8,\n",
    "                   weight_init=\"glorot\",\n",
    "                   flatten=True,\n",
    "                  activation=Tanh_final()),\n",
    "            Dense_final(neurons=10, \n",
    "                  activation=Linear_final())],\n",
    "            loss = SoftmaxCrossEntropy(), \n",
    "seed=20190402)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the same MNIST dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(-1,28*28)\n",
    "test_images = test_images.reshape(-1,28*28)\n",
    "\n",
    "X_train = (train_images - np.mean(train_images)) / np.std(train_images)\n",
    "X_test = (test_images - np.mean(test_images)) / np.std(test_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.zeros((train_labels.size, train_labels.max()+1))\n",
    "y_train[np.arange(train_labels.size), train_labels] = 1\n",
    "\n",
    "y_test = np.zeros((test_labels.size, train_labels.max()+1))\n",
    "y_test[np.arange(test_labels.size), test_labels] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_conv = X_train.reshape(-1,1,28,28)\n",
    "X_test_conv = X_test.reshape(-1,1,28,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = SGDMomentum2(0.2,0.05,\"exponential\", 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer_final(model, optimizer)\n",
    "trainer.fit(X_train_conv, y_train, X_test_conv,y_test,batch_size=64,epochs=2, eval_every=1,seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
