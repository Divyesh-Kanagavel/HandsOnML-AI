{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No valid output stream.\n"
     ]
    }
   ],
   "source": [
    "basepath = \"aclImdb\" # to update after dataset download\n",
    "labels = {\"pos\":1, \"neg\":0}\n",
    "pbar  =pyprind.ProgBar(50000)\n",
    "df = pd.DataFrame()\n",
    "for s in (\"test\",\"train\"):\n",
    "    for l in (\"pos\",\"neg\"):\n",
    "        path = os.path.join(basepath,s,l)\n",
    "        for file in os.listdir(path):\n",
    "            with open(os.path.join(path, file), \"r\", encoding=\"utf-8\") as infile:\n",
    "                txt = infile.read()\n",
    "            df = pd.concat([df,pd.DataFrame([[txt, labels[l]]])], ignore_index=True) # df.append is now deprecated\n",
    "            pbar.update()\n",
    "df.columns= [\"review\", \"sentiment\"]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshuffling rows\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv(\"movie_data.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file read check\n",
    "\n",
    "df = pd.read_csv(\"movie_data.csv\",encoding=\"utf-8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I can't add an awful lot to the positive reviews already on here - great acting, balanced writing, multi-faceted characters, a great anti-hero in Tony, great commentary on millennial American life. The integral use of psychiatry coupled with Tony's mother issues are especially fresh and humorous. Several other characters add a lot of depth - Hesh's interesting history as an outsider muscling in, Ralphie's total irredeemable viciousness, Chris' dual desires in life, and so on.<br /><br />I have to dig into some of the criticisms however, especially the 'it glorifies violence/belittles Italian-Americans' one.Most of the writers and actors are Italian-American, would they attack themselves? There are several positive Italian-American characters - Artie Bucco the chef, Dr. Melfi and her family and the Cusamanos next door to the Sopranos. Indeed, Dr Melfi's ex-husband notes in season 1 that only a tiny minority of Italian-Americans have ever had Mob connections (certainly smaller than the proportion of African-Americans involved in crime, dare I say it. In both cases poverty and lack of opportunity are the biggest causes).<br /><br />Most of the characters don't really choose the life they have; family background or circumstances largely corner them into it. Outsiders (even of Italian stock) who attempt to integrate into it usually meet distressing ends - Matthew and his friend in season 2, for example. If you criticise this show, I assume Frasier made you want to be a psychiatrist, or Will & Grace made you want to go homosexual? Presumably you won't listen to rap music that discusses gangs, or r'n'b which discusses promiscuity, or rock music which discusses drugs (or any other combination)? People aren't as stupid as some of you make out....<br /><br />Not everything is perfect however. A lot of characters have only appeared once, when by all logic they should have been seen or at least mentioned in previous episodes - Tracee the dancer, Meadow's friend Ally, Uncle Junior's ladyfriend (supposedly for 20 years until they split in season 1).\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"review\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words model: \n",
    "To work with ML algorithms, we need numbers. so, the words need to be converted to numbers to train our models and achieve the required objectives. \\\n",
    "words -> numerical feature vectors -> ML model -> output numerical results \\\n",
    "In BOW,  a vocabulary is created which stores the list of unique tokens from the entire set of documents. Then parsing through each document, a feature vector is obtained for words present in it taking vocab as reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to construct bag of words model based on count of words appearing in document, CountVectorizer library in sklearn can be used.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "# a sample doc\n",
    "docs = np.array([\"the sun is shining\", \"the water is sweet\", \"The clouds are beautiful and the sun is shining\"])\n",
    "bag = count.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 8, 'sun': 6, 'is': 4, 'shining': 5, 'water': 9, 'sweet': 7, 'clouds': 3, 'are': 1, 'beautiful': 2, 'and': 0}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 1 1 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 1 1]\n",
      " [1 1 1 1 1 1 1 0 2 0]]\n"
     ]
    }
   ],
   "source": [
    "# our feature vector for three phrases in docs\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array basically gives the count of the token at index i.for example , and has position 0 in the vocab. thus for first two sentences, in indices 0, we have no and, and the value is 0. in third sentence, we have a and and thus we have 1 there.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values in the feature vector are called raw term frequencies tf (t,d), number of times term t occurs in document d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sequence of items created in the vocabulary is basically a 1 gram model. where we assign a number to a word in the dataset. WE could also have n-gram model where we have a n words together assigned to a number. \n",
    "For example 2 gram model could have {\"the sun\", \"sun is\", \"is shining\"}. the n could depend on the application , for spam filtering, n=3,4 works well. for other tasks, we can try with different parameters.\n",
    "\n",
    "setting n is trivial in sklearn. n_gram_range = (2,2) - for 2 gram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word relevancy : Term frequency inverse document frequency\n",
    "Some words occur frequently across multiple documents and thus do not contain useful information. it is better to reduce the influence of these words while making decisions or constructing feature vectors with these words. one way of doing it is using tf - idf (term frequence - inverse document frequency)\n",
    "tf-idf(t,d) = tf(t,d) * idf(t,d)\n",
    "idf(t,d) = $log{\\frac{n_d}{1+df(d,t)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n_d$ denotes the number of documents and df is the number of documents in which the term is found. the +1 in denom is to prevent division by zero error in case. and logarithm makes sure idf does not blow up to high value for lower df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.43 0.56 0.56 0.   0.43 0.  ]\n",
      " [0.   0.   0.   0.   0.36 0.   0.   0.61 0.36 0.61]\n",
      " [0.38 0.38 0.38 0.38 0.22 0.29 0.29 0.   0.45 0.  ]]\n"
     ]
    }
   ],
   "source": [
    "#sklearn tfidf transformer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn computes tf-idf in a slightly different way. \\\n",
    "idf(t,d) = $\\log{\\frac{1+n_d}{1+df(d,t)}}$ \\\n",
    "tf-idf(t,d) = tf(t,d) * (idf(t,d)+1) \\\n",
    "Though raw frequencies may be normalized before passing tfidf transformer, tf-idf transformer normalizes the frequencies while computing tf-idf . By default, l2 normalization is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning text data:\n",
    "The first important step before passing our text into transformers and model is to clean the data and strip it off unwanted elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s a dream.<br /><br />Seriously interesting stuff.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[1,'review'][-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we see that we lot of html tags in the text and this gives no useful information for opinion mining and is in fact a baggage in already expensive nlp. Usually , punctuations can be very helpful in nlp related tasks like sentiment analysis etc. here we are stripping punctuation marks as well. \\\n",
    "Python's regex library can be used to search and remove certain characters from the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_text(text):\n",
    "    text = re.sub('<[^>]*>','',text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                       text)\n",
    "    text= re.sub('[\\W]+',' ',text.lower()) + ' '.join(emoticons).replace('-','')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding emoticons at the end after remaining text may not seem right, but for our application of movie review, we ll be using bag of words model and the order does not matter -> sentiment analysis requires the weights associated with individual words and order does not matter much. it is important in sentence completion etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s a dream seriously interesting stuff '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_text(df.loc[1,'review'][-50:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying across all reviews\n",
    "df[\"review\"] = df[\"review\"].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(11, 19), match='word:cat'>\n"
     ]
    }
   ],
   "source": [
    "# before getting into next topic, we ll see regex\n",
    "# re.search(pat, str) -> search for pat in str and return if found , else return None\n",
    "str = 'an example word:cat!!'\n",
    "match  = re.search(r'word:\\w\\w\\w',str) # \\w matches a word character a-zA-Z0-9_, W catches ny \n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(1, 4), match='iii'> None\n"
     ]
    }
   ],
   "source": [
    "match1 = re.search(r'iii', 'piiig')\n",
    "match2 = re.search(r'igs', 'piiig') # not found, match == None\n",
    "print(match1, match2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(2, 5), match='iig'>\n"
     ]
    }
   ],
   "source": [
    "# . meaning match any character except \\n\n",
    "match = re.search(r'..g', 'piiig')\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "abc\n",
      "@ab\n"
     ]
    }
   ],
   "source": [
    "match1 = re.search(r'\\d\\d\\d', 'p123g')\n",
    "print(match1.group())\n",
    "match2 = re.search(r'\\w\\w\\w', '@@abcd!')\n",
    "print(match2.group())\n",
    "match3 = re.search(r'\\W\\w\\w', '@@abcd!')\n",
    "print(match3.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pii\n",
      "<re.Match object; span=(1, 3), match='ii'>\n"
     ]
    }
   ],
   "source": [
    "#repetion \n",
    "#+ -- 1 or more occurrences of the pattern to its left, e.g. 'i+' = one or more i's\n",
    "#* -- 0 or more occurrences of the pattern to its left\n",
    "#? -- match 0 or 1 occurrences of the pattern to its left\n",
    "#leftmost & largest match is chosen\n",
    "\n",
    "match = re.search(r'pi+', 'piigs')\n",
    "print(match.group())\n",
    "match = re.search(r'i+', 'piigiiii') # found, match.group() == \"ii\"\n",
    "print(match)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * -> meaning, pattern to its left can be matched 0 or more times, even zero is allowed\n",
    "\n",
    "\n",
    "match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx1 2   3xx') # found, match.group() == \"1 2   3\"\n",
    "match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx12  3xx') # found, match.group() == \"12  3\"\n",
    "match = re.search(r'\\d\\s*\\d\\s*\\d', 'xx123xx') # found, match.group() == \"123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "bar\n",
      "barley\n"
     ]
    }
   ],
   "source": [
    "match = re.search(r'^b\\w+', 'foobar') # expected : no match. ^ denotes start of string, foobar does not begin with b\n",
    "print(None if match == None else match.group())\n",
    "match = re.search(r'b\\w+', 'foobar') # expected bar\n",
    "print(None if match == None else match.group())\n",
    "match = re.search(r'^b\\w+', 'barley') # expected barley because barley does begin with b\n",
    "print(None if match == None else match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b@google\n"
     ]
    }
   ],
   "source": [
    "# email \n",
    "str = 'purple alice-b@google.com monkey dishwasher'\n",
    "#simple approach\n",
    "match = re.search(r'\\w+@\\w+',str)\n",
    "print(None if match == None else match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "does not produce full email id, because -, . are not included in \\w. \\\n",
    "A set of characters to search for can be included in a square bracket. this can solve a lot of problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice-b@google.com\n"
     ]
    }
   ],
   "source": [
    "match = re.search(r'[\\w.-]+@[\\w.-]+',str) # this included . and - in the character search\n",
    "print(None if match == None else match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some square bracket rules:\n",
    "1. to include a range of characters, put a dash in between. for example [a-z]\n",
    "2. to include dash as a character, put it in the end.for example [ab-]\n",
    "3. ^ in the beginning will excluded characters. for example [^ab] meaning chars except a and b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group extraction\n",
    "The \"group\" feature of a regular expression allows you to pick out parts of the matching text. Suppose for the emails problem that we want to extract the username and host separately. To do this, add parentheses ( ) around the username and host in the pattern, like this: r'([\\w.-]+)@([\\w.-]+)'. In this case, the parentheses do not change what the pattern will match, instead they establish logical \"groups\" inside of the match text. On a successful search, match.group(1) is the match text corresponding to the 1st left parentheses, and match.group(2) is the text corresponding to the 2nd left parentheses. The plain match.group() is still the whole match text as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice-b@google.com\n",
      "alice-b\n",
      "google.com\n"
     ]
    }
   ],
   "source": [
    "# group extraction\n",
    "str = 'purple alice-b@google.com monkey dishwasher'\n",
    "match = re.search(r'([\\w.-]+)@([\\w.-]+)',str)\n",
    "if match:\n",
    "    print(match.group()) # the full match\n",
    "    print(match.group(1)) # match group 1\n",
    "    print(match.group(2)) # match group 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('alice', 'google.com')\n",
      "('bob', 'abc.com')\n"
     ]
    }
   ],
   "source": [
    "# findall() just like search but finds all matches within the string\n",
    "\n",
    "str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n",
    "emails = re.findall(r'([\\w.-]+)@([\\w.-]+)',str)\n",
    "for email in emails:\n",
    "    print(email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#include <stdio.h>\n",
      "#include <string.h>\n",
      "int trim(ch\n"
     ]
    }
   ],
   "source": [
    "f  = open(r'/Users/divyeshkanagavel/Desktop/DSA_cpp/programming/c_programs/trim.c','r',encoding='utf-8')\n",
    "text = f.read()\n",
    "print(text[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#include <stdio.h>', '#include <string.h>']\n"
     ]
    }
   ],
   "source": [
    "# get the includes from the c file\n",
    "\n",
    "match = re.findall(r'#\\w+\\s[\\w.<>]+',text)\n",
    "print(match)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Greedy vs non-greedy :\n",
    "By default , regex searches are greedy. suppose you have a text with tags, say html code \\\n",
    "<b>foo</b> or <i>hi</i>. if we give something like <.*> will get entire line <b>foo</b>. to get just <b>, need to enable non-greedy option. this can be done with a question mark. <.*?>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>foo</b>\n"
     ]
    }
   ],
   "source": [
    "text= \"<b>foo</b>\"\n",
    "match =re.search(r'<.*>', text)\n",
    "print(match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<b>\n"
     ]
    }
   ],
   "source": [
    "text= \"<b>foo</b>\"\n",
    "match =re.search(r'<.*?>', text)\n",
    "print(match.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitution:\n",
    "\n",
    "The re.sub(pat, replacement, str) function searches for all the instances of pattern in the given string, and replaces them. The replacement string can include '\\1', '\\2' which refer to the text from group(1), group(2), and so on from the original matching text.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('alice', 'google.com'), ('bob', 'abc.com')]\n",
      "purple alice@yahoo.com, blah monkey bob@yahoo.com blah dishwasher\n"
     ]
    }
   ],
   "source": [
    "str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'\n",
    "print(re.findall(r'([\\w\\.-]+)@([\\w\\.-]+)', str))\n",
    "\n",
    "print(re.sub(r'([\\w\\.-]+)@([\\w\\.-]+)',r'\\1@yahoo.com',str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : babyname Exercises in Google developer Regex tutorials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'like', 'coding,', 'weight', 'lifting', 'and', 'playing', 'chess!']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# back to NLP\n",
    "\n",
    "#processing documents into tokens\n",
    "\n",
    "#simple tokenizer : split text at its whitespace\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "#example\n",
    "tokenizer(\"I like coding, weight lifting and playing chess!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful technique in the context of tokenization is word stemming. that is transforming a word into its root form. It allows us to map words which have same roots to the root. Porter stemmer algorithm is one of the first known stemming algorithm. NLTK library has it implemented in python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'coding,', 'weight', 'lift', 'and', 'play', 'chess!']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "tokenizer_porter(\"I like coding, weight lifting and playing chess!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lancaster and snowball stemmer are advanced stemming algorithms which build on top of stemming algorithm and they are found in the library. Stemming can sometimes result in non-English words like thu . lemmatization is the process of having grammatically correct words in the dataset -> this is computationally expensive and usually both lemmatization and stemming can have similar performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop word removal : Stop words are those words which are extremely common and appear in many documents and thus do not help much in the classification of documents or sentiment analysis and hence can be removed from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/divyeshkanagavel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like', 'coding,', 'weight', 'lift', 'play', 'chess!']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "[w for w in tokenizer_porter(\"I like coding, weight lifting and playing chess!\") if w not in stop]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! once we have the tokenizer, we can go ahead and train a model on this dataset and see if we can analyze the sentiments successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000,'review'].values\n",
    "y_train = df.loc[:25000,'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:,'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,lowercase=False,preprocessor=None)\n",
    "param_grid = [{'vect__ngram_range':[(1,1)],'vect__stop_words':[stop,None], 'vect__tokenizer':[tokenizer, tokenizer_porter],'clf__penalty':['l1','l2'],'clf__C':[1.0,10.0,100.0]},\n",
    "              {'vect__ngram_range':[(1,1)], 'vect__stop_words':[stop,None],'vect__tokenizer':[tokenizer, tokenizer_porter], 'vect__use_idf':[False], 'vect__norm':[None], 'clf__penalty':['l1','l2'], 'clf__C':[1.0,10.0,100.0]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'ha', 'hi', \"it'\", 'onc', 'onli', 'ourselv', \"she'\", \"should'v\", 'themselv', 'thi', 'veri', 'wa', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  warnings.warn(\n",
      "/Users/divyeshkanagavel/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lr_tfidf \u001b[38;5;241m=\u001b[39m Pipeline([(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvect\u001b[39m\u001b[38;5;124m'\u001b[39m, tfidf),(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclf\u001b[39m\u001b[38;5;124m'\u001b[39m,LogisticRegression(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))])\n\u001b[1;32m      2\u001b[0m gs_lr_tfidf \u001b[38;5;241m=\u001b[39m GridSearchCV(lr_tfidf, param_grid, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m gs_lr_tfidf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[1;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    894\u001b[0m     )\n\u001b[1;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m--> 898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_search(evaluate_candidates)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_grid))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[1;32m    842\u001b[0m         )\n\u001b[1;32m    843\u001b[0m     )\n\u001b[0;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    847\u001b[0m         clone(base_estimator),\n\u001b[1;32m    848\u001b[0m         X,\n\u001b[1;32m    849\u001b[0m         y,\n\u001b[1;32m    850\u001b[0m         train\u001b[38;5;241m=\u001b[39mtrain,\n\u001b[1;32m    851\u001b[0m         test\u001b[38;5;241m=\u001b[39mtest,\n\u001b[1;32m    852\u001b[0m         parameters\u001b[38;5;241m=\u001b[39mparameters,\n\u001b[1;32m    853\u001b[0m         split_progress\u001b[38;5;241m=\u001b[39m(split_idx, n_splits),\n\u001b[1;32m    854\u001b[0m         candidate_progress\u001b[38;5;241m=\u001b[39m(cand_idx, n_candidates),\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_and_score_kwargs,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[38;5;129;01min\u001b[39;00m product(\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;28menumerate\u001b[39m(candidate_params), \u001b[38;5;28menumerate\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups))\n\u001b[1;32m    859\u001b[0m     )\n\u001b[1;32m    860\u001b[0m )\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    867\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(iterable_with_config)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretrieve()\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout))\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m future\u001b[38;5;241m.\u001b[39mresult(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         waiter\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr_tfidf = Pipeline([('vect', tfidf),('clf',LogisticRegression(random_state=0))])\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid, scoring='accuracy',cv=5,verbose=1,n_jobs=-1)\n",
    "gs_lr_tfidf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x12fd30900>} \n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular classifier for movie review and nlp classification documents is naive Bayes classifier. it is simple to implement, computationally efficient and works well for small datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of core learning : Doing grid search with a bunch of parameters on a dataset of 50000 reviews is quite expensive, took a lot of time to run. the real world datasets are even bigger and sometimes don't even fit in device memory. So, we do something called out of core learning, where gradients are updated after processing a batch of data instead of updating it one sample at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will make use of the partial_fit function of the SGDClassifier in scikit-learn to stream the documents directly from our local drive, and train a logistic regression model using small mini-batches of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "def tokenizer(text):\n",
    "\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)',\n",
    "                       text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) \\\n",
    "       + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return tokenized\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_docs(path):\n",
    "    with open(path,'r',encoding='utf-8') as csv:\n",
    "        next(csv) # skip the header\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\"I can\\'t add an awful lot to the positive reviews already on here - great acting, balanced writing, multi-faceted characters, a great anti-hero in Tony, great commentary on millennial American life. The integral use of psychiatry coupled with Tony\\'s mother issues are especially fresh and humorous. Several other characters add a lot of depth - Hesh\\'s interesting history as an outsider muscling in, Ralphie\\'s total irredeemable viciousness, Chris\\' dual desires in life, and so on.<br /><br />I have to dig into some of the criticisms however, especially the \\'it glorifies violence/belittles Italian-Americans\\' one.Most of the writers and actors are Italian-American, would they attack themselves? There are several positive Italian-American characters - Artie Bucco the chef, Dr. Melfi and her family and the Cusamanos next door to the Sopranos. Indeed, Dr Melfi\\'s ex-husband notes in season 1 that only a tiny minority of Italian-Americans have ever had Mob connections (certainly smaller than the proportion of African-Americans involved in crime, dare I say it. In both cases poverty and lack of opportunity are the biggest causes).<br /><br />Most of the characters don\\'t really choose the life they have; family background or circumstances largely corner them into it. Outsiders (even of Italian stock) who attempt to integrate into it usually meet distressing ends - Matthew and his friend in season 2, for example. If you criticise this show, I assume Frasier made you want to be a psychiatrist, or Will & Grace made you want to go homosexual? Presumably you won\\'t listen to rap music that discusses gangs, or r\\'n\\'b which discusses promiscuity, or rock music which discusses drugs (or any other combination)? People aren\\'t as stupid as some of you make out....<br /><br />Not everything is perfect however. A lot of characters have only appeared once, when by all logic they should have been seen or at least mentioned in previous episodes - Tracee the dancer, Meadow\\'s friend Ally, Uncle Junior\\'s ladyfriend (supposedly for 20 years until they split in season 1).\"',\n",
       " 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(stream_docs(path='movie_data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch processing\n",
    "def minibatch(docs_stream, size):\n",
    "    docs, y = [],[]\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text,label = next(docs_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count vectorizer needs the entire vocabulary of dataset in memory and tfidf vectorizer needs access to all documents to compute idf frequencies. there is another vectorizer called the hash vectorizer which can work with out of core learning. Hash vectorizer is data independent, it takes the string and based on the string assigns a feature vector using a hashing function. having higher feature vector can minimize hash collisions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "vect = HashingVectorizer(decode_error=\"ignore\",n_features = 2**21, preprocessor=None,tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log_loss', random_state=1,max_iter=1)\n",
    "\n",
    "doc_stream = stream_docs(path='movie_data.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No valid output stream.\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0,1])\n",
    "\n",
    "for _ in range(45):\n",
    "    X_train, y_train = minibatch(doc_stream, size=1000)\n",
    "    \n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle # for numpy array joblib does a good job in storing weights. but pickle is more standardized\n",
    "import os\n",
    "dest = os.path.join('movie_classifier', 'pkl_objects')\n",
    "if not os.path.exists(dest):\n",
    "    os.makedirs(dest)\n",
    "pickle.dump(stop, open(os.path.join(dest,'stopwords.pkl'), 'wb'),protocol=4)\n",
    "pickle.dump(clf, open(os.path.join(dest,'classifier.pkl'), 'wb'),protocol=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be deploying this trained model on a public web server and for that we need to dump the model and some other objects into a file which can then be used in another jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there is no need to dump the hashmap vectorizer because it can be written as a python script and loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit starts from random parameters and after learning from the entire dataset, we get the correct weights. \\\n",
    "partial_fit updates the parameters without clearing or starting from scratch. the weights are merely updated based on gradients from new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modern alternative to bag of words is word2vec, which is an unsupervised neural network algorithm which is trained on corpus of data to learn representations between words. the feature vectors are calculated such that similarity in words can be easily mapped to vector transformations like addition, scaling etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modelling with LDA : Latent Dirichlet Allocation -> not to be confused with Linear Discriminant Analysis (another supervised learning algorithm which is used in dimensionality reduction). \\\n",
    "Latent Dirichlet Allocation is a unsupervised learning algorithm which is used to assign topics to unlabelled documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA : generative probabilistic model that tries to find groups of words that appear frequently together across different documents.\n",
    "Input to LDA : bag of words model [a matrix] \\\n",
    "LDA decomposes this matrix into two matrices : document to topic matrix, a word to topic matrix\n",
    "these two can be multiplied together to get back the original bag of words matrix with very little loss of error.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"movie_data.csv\", encoding='utf-8')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words = 'english', max_df=.1, max_features = 5000) #max_df - maximum document frequency - if a word occurs in  more than 10 percent of the documents , omit it pls\n",
    "\n",
    "X = count.fit_transform(df['review'].values) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation \n",
    "lda = LatentDirichletAllocation(n_components = 10 , random_state = 123,learning_method='batch') # batch meaning access to full dataset and better accuracy , if learning_metjhod is set to learning, lower accuracy but improved performance.\n",
    "X_topics = lda.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scikit-learn library's implementation of LDA uses the Expectation- Maximization (EM) algorithm to update its parameter estimates iteratively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5000)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes script awful stupid\n",
      "Topic 2:\n",
      "family mother father children girl\n",
      "Topic 3:\n",
      "war american dvd music history\n",
      "Topic 4:\n",
      "human audience cinema art feel\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house blood sex gore\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode tv episodes season\n",
      "Topic 9:\n",
      "book version original effects read\n",
      "Topic 10:\n",
      "action fight guy kids fun\n"
     ]
    }
   ],
   "source": [
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(\"Topic %d:\" % (topic_idx+1))\n",
    "    print(\" \".join([feature_names[i]\n",
    "                for i in topic.argsort()\\\n",
    "                    [:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_topics.shape # (num_documents, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "horror = X_topics[:,5].argsort()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1766\n",
      "\n",
      "Horror movie #1:\n",
      "Emilio Miraglia's first Giallo feature, The Night Evelyn Came Out of the Grave, was a great combination of Giallo and Gothic horror - and this second film is even better! We've got more of the Giallo side of the equation this time around, although Miraglia doesn't lose the Gothic horror stylings tha ...\n",
      "7918\n",
      "\n",
      "Horror movie #2:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n",
      "44633\n",
      "\n",
      "Horror movie #3:\n",
      "This film marked the end of the \"serious\" Universal Monsters era (Abbott and Costello meet up with the monsters later in \"Abbott and Costello Meet Frankentstein\"). It was a somewhat desparate, yet fun attempt to revive the classic monsters of the Wolf Man, Frankenstein's monster, and Dracula one \"la ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:,5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print(movie_idx)\n",
    "    print('\\nHorror movie #%d:' % (iter_idx + 1))\n",
    "    print(df['review'][movie_idx][:300],'...')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
